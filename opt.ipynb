{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import *\n",
    "import trimesh\n",
    "from manotorch.manolayer import ManoLayer\n",
    "from pysdf import SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = '/root/code/seqs/0303_data'\n",
    "obj_path = '/root/code/seqs/object'\n",
    "contactgen_path = 'final_results/contactgen_500_2000/'\n",
    "# obj_pose_path ''\n",
    "# path = join('save/0303_stage0_1obj/samples_0303_stage0_1obj_000030000_seed10_predefined/pred_obj',f'{seq}_pred_obj_and_goal.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把所有的hand trans转为需要减去joint0的格式\n",
    "\n",
    "align_hand_param.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = sorted(os.listdir(contactgen_path))\n",
    "manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano', side='right')\n",
    "hand_faces = manolayer.th_faces\n",
    "\n",
    "# joints = output.joints  + hand_trans.unsqueeze(1)\n",
    "# hand_verts = output.verts  + hand_trans.unsqueeze(1)\n",
    "# joints = output.joints - output.joints[:, 0].unsqueeze(1) + hand_trans.unsqueeze(1)\n",
    "for seq in seqs:\n",
    "    res_path = join(contactgen_path,seq,'results')\n",
    "    ids = sorted(os.listdir(res_path))\n",
    "    for id in ids:\n",
    "        idpath = join(res_path,id)\n",
    "        old_hand_params = torch.tensor(np.load(join(idpath,'new_hand_params.npy')))\n",
    "        print(old_hand_params.shape)\n",
    "        hand_trans = old_hand_params[:3].unsqueeze(0)\n",
    "        hand_theta = old_hand_params[3:51].unsqueeze(0)\n",
    "        mano_beta = old_hand_params[51:].unsqueeze(0)\n",
    "        output = manolayer(hand_theta, mano_beta)\n",
    "        joint_offset = output.joints[:, 0].unsqueeze(1)\n",
    "        new_hand_trans = hand_trans + joint_offset\n",
    "        align_hand_params = old_hand_params.squeeze(0)\n",
    "        align_hand_params[:3] = new_hand_trans.squeeze(0)\n",
    "        align_hand_params = align_hand_params.numpy()\n",
    "        np.save(join(idpath,'align_hand_params.npy'), align_hand_params)\n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算优化需要的hand T和obj_tip参数 之前的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = '/root/code/seqs/gazehoi_list_test_0303.txt'\n",
    "# with open(datapath,'r') as f:\n",
    "#     info_list = f.readlines()\n",
    "# seqs = []\n",
    "# for info in info_list:\n",
    "#     seq = info.strip()\n",
    "#     seqs.append(seq)\n",
    "\n",
    "data_dict = {}\n",
    "stage1_res = np.load('save/8000_DSG/samples_final_000008000_seed10_new_dsg750_gaze/results.npy',allow_pickle=True).item()\n",
    "print(stage1_res.keys())\n",
    "hand_motions = torch.tensor(stage1_res['motion']).float()\n",
    "seqs = stage1_res['seqs']\n",
    "hint = torch.tensor(stage1_res['hint']).float()\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    seq = seqs[i]\n",
    "    offset = torch.tensor(np.load(join(contactgen_path,seq,'offset.npy')))\n",
    "    seq_path = join(rootpath,seq)\n",
    "    meta_path = join(seq_path,'meta.pkl')\n",
    "    with open(meta_path, 'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    gaze_obj = meta['gaze_obj']\n",
    "    gt_hand_params = np.load(join(seq_path,'mano/poses_right.npy'))\n",
    "    mano_beta = torch.tensor(gt_hand_params[0,51:])\n",
    "    obj_mesh = trimesh.load(join(obj_path,gaze_obj,'simplified_scan_processed.obj'))\n",
    "    obj_verts = torch.tensor(obj_mesh.vertices).float()\n",
    "    obj_faces = obj_mesh.faces\n",
    "\n",
    "    stage0_res = np.load(join('save/0303_stage0_1obj/samples_0303_stage0_1obj_000030000_seed10_predefined/pred_obj',f'{seq}_pred_obj_and_goal.npy'),allow_pickle=True).item()\n",
    "    # print(stage0_res.keys())\n",
    "    obj_pose = torch.tensor(stage0_res['pred_obj_pose']).float()\n",
    "    goal_index = stage0_res['goal_index']\n",
    "    if goal_index<=59:\n",
    "        goal_hand_pose = hint[i,goal_index]\n",
    "        \n",
    "    else:\n",
    "        goal_hand_pose = hint[i,-1]\n",
    "        # print(seq)\n",
    "    # print(seq,goal_hand_pose)\n",
    "    # goal_hand_pose[:3] = goal_hand_pose[:3] + offset\n",
    "    stage2_obj_pose = obj_pose[goal_index:]\n",
    "    goal_obj_pose = obj_pose[goal_index]\n",
    "\n",
    "    trans_obj_verts = get_trans_obj_verts(obj_verts,obj_pose)\n",
    "    goal_obj_verts = trans_obj_verts[0]\n",
    "\n",
    "    stage2_hand_T = get_hand_T(stage2_obj_pose, trans_obj_verts[goal_index:], goal_hand_pose)\n",
    "    np.save(join(seq_path,'stage2_hand_T.npy'),stage2_hand_T.numpy())\n",
    "    tip_obj_ids, tip_obj_dis,hand_verts,hand_faces = get_tip_obj(goal_obj_pose, goal_obj_verts, goal_hand_pose, mano_beta)\n",
    "    # print(stage2_hand_T.shape)\n",
    "    # tip_obj = torch.cat(tip_obj_ids,tip_obj_dis)\n",
    "    # print(tip_obj.shape)\n",
    "    np.save(join(seq_path,'tip_obj_ids.npy'), tip_obj_ids.numpy())\n",
    "    np.save(join(seq_path,'tip_obj_dis.npy'), tip_obj_dis.numpy())\n",
    "    \n",
    "    # print(np.load(join(seq_path,'tips_closest_id_and_dis.npy')))\n",
    "    \n",
    "    # render = {'hand_verts':hand_verts.squeeze(0).numpy(),'hand_faces':hand_faces,'obj_verts':goal_obj_verts.numpy(),'obj_faces':obj_faces}\n",
    "    # np.save(join(f'final_results/check_opt_offset/{seq}_for_render.npy'),render)\n",
    "    # break\n",
    "    print(\"#################\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tip_obj(goal_obj_pose, obj_verts, goal_hand_pose,mano_beta):\n",
    "    manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano', side='right')\n",
    "    hand_faces = manolayer.th_faces\n",
    "    hand_trans = goal_hand_pose[:3].unsqueeze(0)\n",
    "    hand_theta = goal_hand_pose[3:51].unsqueeze(0)\n",
    "    mano_beta = mano_beta.unsqueeze(0)\n",
    "    output = manolayer(hand_theta, mano_beta)\n",
    "    # joints = output.joints  + hand_trans.unsqueeze(1)\n",
    "    # hand_verts = output.verts  + hand_trans.unsqueeze(1)\n",
    "    joints = output.joints - output.joints[:, 0].unsqueeze(1) + hand_trans.unsqueeze(1)\n",
    "    hand_verts = output.verts - output.joints[:, 0].unsqueeze(1) + hand_trans.unsqueeze(1)\n",
    "    print(hand_verts.shape)\n",
    "    tips = [15,3,6,12,9]\n",
    "    tips_T = joints[:,tips].squeeze(0)\n",
    "    print(tips_T.shape)\n",
    "    dis = torch.cdist(tips_T,obj_verts)\n",
    "    tip_obj_dis,tip_obj_ids = dis.min(dim=-1)\n",
    "    print(tip_obj_ids)\n",
    "    print(tip_obj_dis)\n",
    "\n",
    "\n",
    "    \n",
    "    return tip_obj_ids, tip_obj_dis,hand_verts,hand_faces\n",
    "    \n",
    "def get_hand_T(stage2_obj_pose, trans_obj_verts, goal_hand_pose):\n",
    "    stage2_len = stage2_obj_pose.shape[0]\n",
    "    update_hand_T = torch.zeros((stage2_len,3))\n",
    "    update_hand_T[0] = goal_hand_pose[:3]\n",
    "\n",
    "\n",
    "    obj_R = stage2_obj_pose[:,:3,:3]\n",
    "    obj_T = stage2_obj_pose[:,:3,3]\n",
    "    reference_obj_rot = obj_R[0]\n",
    "\n",
    "    # trans_obj_verts = get_trans_obj_verts(obj_verts,stage2_obj_pose)\n",
    "    goal_obj_verts = trans_obj_verts[0]\n",
    "    goal_hand_trans = goal_hand_pose[:3].unsqueeze(0)\n",
    "    dis = torch.cdist(goal_hand_trans,goal_obj_verts)\n",
    "    min_dis,obj_idx = dis.min(dim=-1)\n",
    "    contact_normal = goal_hand_trans - goal_obj_verts[obj_idx]\n",
    "    \n",
    "    for i in range(1, stage2_len):\n",
    "        relative_rot_mat = torch.matmul(obj_R[i], reference_obj_rot.inverse())\n",
    "        curr_contact_normal = torch.matmul(relative_rot_mat, contact_normal.T).squeeze(-1)\n",
    "        update_hand_T[i] = trans_obj_verts[i, obj_idx] + curr_contact_normal\n",
    "    return update_hand_T\n",
    "\n",
    "def get_trans_obj_verts(obj_verts,obj_pose):\n",
    "    \"\"\"\n",
    "    obj_verts: (N,3) -- T,N\n",
    "    obj_pose: (T,3,4)\n",
    "    \"\"\"\n",
    "    nf = obj_pose.shape[0]\n",
    "    N = obj_verts.shape[0]\n",
    "    # obj_pose = obj_pose.unsqueeze(1).repeat(1,N,1,1)\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    \n",
    "    obj_verts = obj_verts.unsqueeze(0).repeat(nf,1,1)\n",
    "    trans_obj_verts = torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T\n",
    "    return trans_obj_verts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修正后 获得优化需要的hand T和tip obj，for video visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['motion', 'lengths', 'hint', 'seqs', 'num_samples', 'num_repetitions'])\n",
      "torch.Size([130, 60, 51])\n",
      "torch.Size([1, 778, 3])\n",
      "torch.Size([5, 3])\n",
      "tensor([ 7985, 11360,  4571, 10392,  2273])\n",
      "tensor([0.0387, 0.0335, 0.0367, 0.0200, 0.0612])\n",
      "#################\n"
     ]
    }
   ],
   "source": [
    "# datapath = '/root/code/seqs/gazehoi_list_test_0303.txt'\n",
    "# with open(datapath,'r') as f:\n",
    "#     info_list = f.readlines()\n",
    "# seqs = []\n",
    "# for info in info_list:\n",
    "#     seq = info.strip()\n",
    "#     seqs.append(seq)\n",
    "\n",
    "data_dict = {}\n",
    "stage1_res = np.load('save/0303_stage1_simple/samples_0303_stage1_simple_000008000_seed10_0312_val_gaze_align/results.npy',allow_pickle=True).item()\n",
    "print(stage1_res.keys())\n",
    "hand_motions = torch.tensor(stage1_res['motion']).float()\n",
    "print(hand_motions.shape)\n",
    "seqs = stage1_res['seqs']\n",
    "hint = torch.tensor(stage1_res['hint']).float()\n",
    "select_seqs = ['1354']\n",
    "goal_index = 35\n",
    "for i in range(len(seqs)):\n",
    "    seq = seqs[i]\n",
    "    if seq not in select_seqs:\n",
    "        continue\n",
    "    offset = torch.tensor(np.load(join(contactgen_path,seq,'offset.npy')))\n",
    "    seq_path = join(rootpath,seq)\n",
    "    meta_path = join(seq_path,'meta.pkl')\n",
    "    with open(meta_path, 'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    gaze_obj = meta['gaze_obj']\n",
    "    gt_hand_params = np.load(join(seq_path,'mano/poses_right.npy'))\n",
    "    mano_beta = torch.tensor(gt_hand_params[0,51:])\n",
    "    obj_mesh = trimesh.load(join(obj_path,gaze_obj,'simplified_scan_processed.obj'))\n",
    "    obj_verts = torch.tensor(obj_mesh.vertices).float()\n",
    "    obj_faces = obj_mesh.faces\n",
    "\n",
    "    stage0_res = np.load(join('save/0303_stage0_1obj/samples_0303_stage0_1obj_000030000_seed10_predefined/pred_obj',f'{seq}_pred_obj_and_goal.npy'),allow_pickle=True).item()\n",
    "    obj_pose = torch.tensor(stage0_res['pred_obj_pose']).float()\n",
    "    \n",
    "    goal_hand_pose = hand_motions[i,goal_index]\n",
    "    stage2_obj_pose = obj_pose[goal_index:]\n",
    "    goal_obj_pose = obj_pose[goal_index]\n",
    "\n",
    "    trans_obj_verts = get_trans_obj_verts(obj_verts,obj_pose)\n",
    "    goal_obj_verts = trans_obj_verts[goal_index]\n",
    "\n",
    "    stage2_hand_T = get_hand_T(stage2_obj_pose, trans_obj_verts[goal_index:], goal_hand_pose)\n",
    "    np.save(join(seq_path,'vis_stage2_hand_T.npy'),stage2_hand_T.numpy())\n",
    "    tip_obj_ids, tip_obj_dis,hand_verts,hand_faces = get_tip_obj(goal_obj_pose, goal_obj_verts, goal_hand_pose, mano_beta)\n",
    "    # print(stage2_hand_T.shape)\n",
    "    # tip_obj = torch.cat(tip_obj_ids,tip_obj_dis)\n",
    "    # print(tip_obj.shape)\n",
    "    np.save(join(seq_path,'vis_tip_obj_ids.npy'), tip_obj_ids.numpy())\n",
    "    np.save(join(seq_path,'vis_tip_obj_dis.npy'), tip_obj_dis.numpy())\n",
    "    \n",
    "    # print(np.load(join(seq_path,'tips_closest_id_and_dis.npy')))\n",
    "    \n",
    "    render = {'hand_verts':hand_verts.squeeze(0).numpy(),'hand_faces':hand_faces,'obj_verts':goal_obj_verts.numpy(),'obj_faces':obj_faces}\n",
    "    new_path = f'final_results/check_vis/{seq}'\n",
    "    os.makedirs(new_path,exist_ok=True)\n",
    "    np.save(join(f'final_results/check_vis/{seq}/{seq}_for_render.npy'),render)\n",
    "    # break\n",
    "    print(\"#################\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
