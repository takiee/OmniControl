{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mdm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pysdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3686136/1931170917.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpysdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrimesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmanotorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanolayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mManoLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pysdf'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from pysdf import SDF\n",
    "import trimesh\n",
    "from manotorch.manolayer import ManoLayer\n",
    "from tqdm import *\n",
    "\n",
    "def convert_to_obj_frame(pc, obj_rot, obj_trans):\n",
    "    pc = (obj_rot.T @ (pc - obj_trans).T).T\n",
    "    return pc\n",
    "\n",
    "def get_close(contact):\n",
    "        return np.sum(np.abs(contact[np.abs(contact)<0.01]))\n",
    "\n",
    "datapath = '/root/code/seqs/1205_data/'\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "manolayer = ManoLayer(\n",
    "    mano_assets_root='/root/code/CAMS/data/mano_assets/mano',\n",
    "    side='right'\n",
    ")\n",
    "\n",
    "mano_knuckles_file = '/root/code/CAMS/data/preparation/mano_parts_1.pt' # MANO part file of knuckles\n",
    "mano_parts_d = torch.load(mano_knuckles_file) # 1552面\n",
    "verts_part_d = np.zeros(778, dtype=np.int32)\n",
    "mano_faces = manolayer.get_mano_closed_faces()\n",
    "for face_id in range(mano_faces.shape[0]):\n",
    "    verts_part_d[mano_faces[face_id, 0]] = mano_parts_d[face_id]\n",
    "    verts_part_d[mano_faces[face_id, 1]] = mano_parts_d[face_id]\n",
    "    verts_part_d[mano_faces[face_id, 2]] = mano_parts_d[face_id]\n",
    "verts_part_d = torch.from_numpy(verts_part_d)\n",
    "\n",
    "tips = [15,3,6,12,9]\n",
    "\n",
    "seqs = sorted(os.listdir(datapath))\n",
    "grasp_id = []\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 185/1378 [19:25<2:13:06,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0185 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 338/1378 [38:16<6:04:50, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0338 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 341/1378 [39:29<6:27:11, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0341 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 381/1378 [46:53<5:19:46, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0381 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 484/1378 [57:34<1:41:45,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0484 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 605/1378 [1:18:13<9:29:38, 44.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0605 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 756/1378 [1:41:31<3:00:53, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0756 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 757/1378 [1:42:49<5:03:44, 29.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0757 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 765/1378 [1:44:39<3:26:49, 20.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0765 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 766/1378 [1:45:53<5:07:17, 30.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0766 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 878/1378 [2:04:31<3:57:33, 28.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0878 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 971/1378 [2:20:38<1:34:40, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0971 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1072/1378 [2:42:51<37:27,  7.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1194/1378 [2:56:15<57:33, 18.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194 failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1378/1378 [3:37:48<00:00,  9.48s/it]  \n"
     ]
    }
   ],
   "source": [
    "grasp_dict = {}\n",
    "seqs = sorted(os.listdir(datapath))\n",
    "for seq in tqdm(seqs):\n",
    "    seq_path = join(datapath,seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "        hand_trans = hand_params[:,:3]\n",
    "        hand_theta = hand_params[:,3:51]\n",
    "        mano_beta = hand_params[:,51:]\n",
    "        active_obj = meta['active_obj']\n",
    "        obj_mesh_path = join(obj_path,active_obj,'simplified_scan_processed.obj')\n",
    "        obj_mesh = trimesh.load(obj_mesh_path)\n",
    "        obj_pose = torch.tensor(np.load(join(seq_path,active_obj+'_pose_trans.npy'))).float()\n",
    "        obj_rot = obj_pose[:,:,:3]\n",
    "        obj_trans = obj_pose[:,:,3]\n",
    "        obj_sdf = SDF(obj_mesh.vertices,obj_mesh.faces)\n",
    "        for i in range(len(hand_params)):\n",
    "            thumb_flag = False\n",
    "            finger_flag = False\n",
    "            save_flag = False\n",
    "            mano_output = manolayer(hand_theta[i].unsqueeze(0), mano_beta[i].unsqueeze(0))\n",
    "            mano_verts = mano_output.verts[0] - mano_output.joints[0, 0] + hand_trans[i]\n",
    "            mano_verts = convert_to_obj_frame(mano_verts.squeeze(),\n",
    "                                                obj_rot[i],\n",
    "                                                obj_trans[i])\n",
    "            # hand_faces = manolayer.get_mano_closed_faces()\n",
    "            hand_contact = obj_sdf(mano_verts)\n",
    "            # print(hand_contact.shape)\n",
    "            if get_close(hand_contact) > 0:\n",
    "                for tip in tips:\n",
    "                    tip_contact = hand_contact[verts_part_d == tip]\n",
    "                    tip_close = get_close(tip_contact)\n",
    "                    if tip == 15:\n",
    "                        if tip_close > 0:\n",
    "                            thumb_flag = True\n",
    "                    else:\n",
    "                        if tip_close > 0 :\n",
    "                            finger_flag = True\n",
    "                if thumb_flag and finger_flag:\n",
    "                    grasp_dict[seq] = i\n",
    "                    # print(seq,i)\n",
    "                    break\n",
    "            else:\n",
    "                if i == len(hand_params) - 1:\n",
    "                    print(seq,\"failed.\")\n",
    "        # break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grasp_dict = {}\n",
    "def get_close(contact):\n",
    "        return np.sum(np.abs(contact[np.abs(contact)<0.03]))\n",
    "fail_seqs = ['0185','0338','0341','0381','0484','0605','0756','0765','0766','0878','0971','1072','1194']\n",
    "for seq in tqdm(fail_seqs):\n",
    "    seq_path = join(datapath,seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "        hand_trans = hand_params[:,:3]\n",
    "        hand_theta = hand_params[:,3:51]\n",
    "        mano_beta = hand_params[:,51:]\n",
    "        active_obj = meta['active_obj']\n",
    "        print(active_obj)\n",
    "        obj_mesh_path = join(obj_path,active_obj,'simplified_scan_processed.obj')\n",
    "        obj_mesh = trimesh.load(obj_mesh_path)\n",
    "        obj_pose = torch.tensor(np.load(join(seq_path,active_obj+'_pose_trans.npy'))).float()\n",
    "        obj_rot = obj_pose[:,:,:3]\n",
    "        obj_trans = obj_pose[:,:,3]\n",
    "        obj_sdf = SDF(obj_mesh.vertices,obj_mesh.faces)\n",
    "        for i in range(len(hand_params)):\n",
    "            thumb_flag = False\n",
    "            finger_flag = False\n",
    "            save_flag = False\n",
    "            mano_output = manolayer(hand_theta[i].unsqueeze(0), mano_beta[i].unsqueeze(0))\n",
    "            mano_verts = mano_output.verts[0] - mano_output.joints[0, 0] + hand_trans[i]\n",
    "            mano_verts = convert_to_obj_frame(mano_verts.squeeze(),\n",
    "                                                obj_rot[i],\n",
    "                                                obj_trans[i])\n",
    "            # hand_faces = manolayer.get_mano_closed_faces()\n",
    "            hand_contact = obj_sdf(mano_verts)\n",
    "            # print(hand_contact.shape)\n",
    "            if get_close(hand_contact) > 0:\n",
    "                for tip in tips:\n",
    "                    tip_contact = hand_contact[verts_part_d == tip]\n",
    "                    tip_close = get_close(tip_contact)\n",
    "                    print(tip,tip_close)\n",
    "                    if tip == 15:\n",
    "                        if tip_close > 0:\n",
    "                            thumb_flag = True\n",
    "                    else:\n",
    "                        if tip_close > 0 :\n",
    "                            finger_flag = True\n",
    "                if thumb_flag and finger_flag:\n",
    "                    grasp_dict[seq] = i\n",
    "                    # print(seq,i)\n",
    "                    break\n",
    "            else:\n",
    "                if i == len(hand_params) - 1:\n",
    "                    print(seq,\"failed.\")\n",
    "        # break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0001': 14, '0003': 14, '0006': 14, '0007': 13, '0008': 16, '0013': 8, '0019': 10, '0020': 19, '0023': 11, '0024': 10, '0026': 7, '0028': 14, '0031': 15, '0032': 14, '0035': 19, '0037': 20, '0038': 14, '0040': 14, '0042': 18, '0044': 35, '0047': 12, '0048': 18, '0051': 10, '0052': 61, '0055': 18, '0056': 20, '0058': 22, '0061': 16, '0062': 34, '0065': 25, '0067': 34, '0068': 13, '0071': 25, '0072': 27, '0075': 47, '0076': 30, '0079': 44, '0080': 47, '0083': 38, '0084': 38, '0087': 38, '0088': 2, '0091': 43, '0092': 42, '0094': 29, '0096': 36, '0099': 19, '0101': 36, '0102': 3, '0103': 43, '0105': 11, '0106': 8, '0107': 8, '0109': 1, '0111': 62, '0112': 29, '0113': 10, '0114': 46, '0115': 38, '0118': 42, '0119': 33, '0145': 52, '0146': 3, '0147': 72, '0148': 7, '0149': 18, '0150': 9, '0155': 17, '0156': 11, '0160': 48, '0161': 59, '0162': 63, '0163': 43, '0164': 64, '0168': 42, '0169': 33, '0170': 46, '0171': 19, '0172': 75, '0176': 26, '0177': 43, '0178': 34, '0179': 14, '0181': 18, '0183': 18, '0186': 49, '0187': 30, '0189': 24, '0191': 26, '0192': 11, '0194': 46, '0195': 20, '0196': 44, '0197': 19, '0198': 44, '0200': 40, '0201': 19, '0203': 27, '0204': 20, '0205': 15, '0206': 43, '0207': 30, '0209': 16, '0210': 37, '0211': 26, '0214': 23, '0215': 1, '0219': 26, '0220': 14, '0221': 22, '0227': 22, '0228': 20, '0229': 36, '0230': 19, '0231': 26, '0232': 47, '0233': 19, '0236': 35, '0238': 38, '0240': 34, '0242': 25, '0243': 35, '0244': 2, '0245': 23, '0246': 38, '0248': 9, '0250': 42, '0251': 33, '0254': 31, '0255': 22, '0257': 28, '0260': 24, '0261': 27, '0264': 30, '0266': 38, '0269': 38, '0271': 23, '0272': 32, '0274': 18, '0277': 33, '0278': 40, '0279': 31, '0282': 5, '0286': 51, '0287': 45, '0290': 19, '0293': 67, '0294': 27, '0296': 23, '0322': 3, '0324': 41, '0327': 57, '0328': 19, '0330': 35, '0331': 34, '0333': 67, '0335': 55, '0336': 48, '0343': 24, '0344': 23, '0346': 43, '0348': 4, '0349': 34, '0351': 31, '0352': 47, '0353': 40, '0354': 21, '0356': 50, '0360': 31, '0364': 19, '0365': 43, '0366': 26, '0367': 29, '0368': 47, '0370': 49, '0371': 34, '0372': 40, '0374': 11, '0376': 26, '0378': 48, '0379': 4, '0382': 26, '0383': 58, '0386': 33, '0387': 32, '0389': 49, '0390': 30, '0392': 55, '0395': 14, '0396': 31, '0397': 41, '0399': 35, '0400': 39, '0401': 36, '0402': 39, '0404': 22, '0405': 29, '0406': 38, '0407': 20, '0408': 29, '0410': 38, '0412': 25, '0413': 36, '0414': 47, '0416': 26, '0417': 44, '0419': 31, '0420': 35, '0421': 25, '0422': 33, '0425': 26, '0427': 29, '0428': 36, '0430': 27, '0432': 21, '0433': 14, '0434': 35, '0435': 27, '0436': 22, '0437': 62, '0439': 24, '0440': 10, '0443': 32, '0448': 2, '0449': 16, '0450': 9, '0451': 14, '0452': 10, '0455': 22, '0483': 15, '0487': 18, '0489': 34, '0490': 25, '0492': 18, '0493': 13, '0494': 19, '0496': 14, '0497': 37, '0499': 11, '0502': 20, '0503': 10, '0505': 14, '0506': 15, '0508': 17, '0510': 16, '0511': 9, '0512': 17, '0513': 14, '0515': 11, '0519': 18, '0522': 11, '0525': 28, '0528': 29, '0530': 19, '0531': 18, '0532': 18, '0533': 25, '0535': 16, '0536': 16, '0538': 19, '0539': 36, '0542': 16, '0543': 30, '0546': 21, '0547': 19, '0548': 12, '0552': 9, '0553': 8, '0554': 13, '0557': 14, '0558': 12, '0560': 21, '0561': 25, '0564': 22, '0565': 39, '0566': 13, '0567': 45, '0569': 15, '0570': 45, '0572': 15, '0573': 32, '0574': 43, '0586': 63, '0587': 99, '0588': 44, '0589': 61, '0590': 64, '0591': 45, '0592': 28, '0593': 17, '0594': 58, '0595': 33, '0596': 45, '0600': 54, '0601': 142, '0602': 37, '0603': 46, '0604': 25, '0606': 37, '0607': 25, '0608': 11, '0609': 36, '0610': 28, '0611': 49, '0612': 42, '0613': 34, '0614': 29, '0615': 71, '0616': 38, '0617': 21, '0618': 6, '0619': 34, '0620': 43, '0621': 25, '0622': 66, '0623': 40, '0624': 40, '0625': 59, '0626': 25, '0627': 26, '0628': 17, '0629': 43, '0630': 26, '0631': 34, '0632': 105, '0633': 37, '0634': 38, '0635': 25, '0636': 49, '0637': 21, '0638': 23, '0639': 28, '0640': 29, '0641': 22, '0642': 27, '0643': 35, '0644': 16, '0645': 43, '0646': 37, '0647': 34, '0648': 39, '0649': 34, '0650': 68, '0651': 49, '0652': 21, '0653': 24, '0656': 16, '0657': 20, '0658': 31, '0659': 22, '0660': 12, '0661': 15, '0662': 17, '0663': 39, '0664': 34, '0665': 30, '0666': 13, '0667': 19, '0668': 32, '0669': 11, '0670': 15, '0671': 26, '0672': 22, '0673': 22, '0674': 9, '0675': 25, '0676': 30, '0677': 38, '0678': 21, '0679': 22, '0681': 33, '0682': 29, '0683': 13, '0685': 23, '0686': 30, '0688': 14, '0689': 7, '0690': 6, '0691': 22, '0692': 10, '0697': 5, '0698': 12, '0728': 72, '0732': 39, '0733': 12, '0734': 21, '0736': 2, '0738': 41, '0743': 55, '0745': 4, '0746': 9, '0748': 38, '0749': 5, '0751': 25, '0753': 31, '0754': 20, '0759': 42, '0761': 11, '0763': 19, '0767': 47, '0768': 0, '0769': 43, '0770': 50, '0771': 12, '0773': 15, '0775': 15, '0776': 2, '0777': 0, '0778': 5, '0779': 9, '0780': 70, '0782': 13, '0783': 0, '0784': 33, '0785': 20, '0786': 37, '0787': 32, '0788': 17, '0790': 6, '0792': 12, '0794': 8, '0796': 29, '0797': 5, '0798': 27, '0799': 20, '0800': 55, '0801': 36, '0802': 50, '0803': 47, '0804': 36, '0805': 25, '0806': 1, '0807': 39, '0808': 4, '0809': 8, '0810': 11, '0813': 49, '0814': 25, '0815': 30, '0816': 13, '0818': 28, '0820': 3, '0821': 24, '0823': 19, '0825': 9, '0827': 19, '0829': 8, '0831': 51, '0832': 16, '0833': 38, '0834': 19, '0835': 46, '0836': 0, '0837': 30, '0839': 49, '0843': 46, '0844': 27, '0845': 47, '0846': 53, '0847': 80, '0849': 64, '0850': 26, '0852': 30, '0854': 43, '0856': 28, '0857': 29, '0858': 22, '0859': 26, '0860': 31, '0862': 47, '0863': 31, '0865': 37, '0867': 103, '0868': 52, '0871': 30, '0872': 73, '0876': 55, '0877': 55, '0879': 30, '0882': 35, '0885': 25, '0886': 27, '0887': 50, '0889': 52, '0890': 13, '0891': 14, '0892': 25, '0893': 35, '0894': 41, '0895': 50, '0896': 45, '0915': 20, '0918': 29, '0919': 39, '0920': 25, '0922': 26, '0924': 25, '0926': 20, '0928': 30, '0929': 26, '0931': 32, '0932': 22, '0933': 108, '0935': 34, '0936': 28, '0937': 83, '0938': 91, '0939': 45, '0940': 34, '0941': 31, '0942': 44, '0943': 38, '0944': 23, '0945': 56, '0946': 34, '0947': 29, '0948': 27, '0949': 41, '0950': 35, '0951': 16, '0952': 31, '0955': 19, '0957': 39, '0959': 42, '0961': 28, '0962': 144, '0965': 24, '0966': 40, '0969': 31, '0970': 48, '0973': 40, '0974': 49, '0976': 36, '0978': 34, '0981': 42, '0982': 35, '0983': 26, '0984': 23, '0985': 34, '0986': 46, '0987': 38, '0988': 38, '0989': 35, '0990': 28, '0991': 33, '0992': 37, '0993': 38, '0994': 57, '0995': 44, '0996': 32, '0997': 24, '0998': 38, '0999': 40, '1000': 24, '1001': 46, '1002': 50, '1003': 45, '1004': 52, '1005': 56, '1006': 40, '1007': 30, '1009': 38, '1010': 43, '1011': 46, '1013': 45, '1015': 27, '1016': 34, '1018': 46, '1019': 28, '1020': 64, '1021': 49, '1024': 46, '1025': 45, '1026': 46, '1027': 45, '1028': 44, '1029': 53, '1030': 34, '1031': 55, '1033': 76, '1036': 72, '1038': 50, '1039': 20, '1041': 47, '1044': 45, '1046': 50, '1047': 37, '1048': 36, '1050': 57, '1052': 36, '1054': 54, '1055': 49, '1056': 38, '1057': 61, '1058': 37, '1059': 35, '1061': 29, '1062': 43, '1132': 51, '1133': 62, '1134': 61, '1135': 47, '1136': 123, '1137': 57, '1140': 45, '1141': 53, '1142': 52, '1143': 46, '1144': 32, '1145': 57, '1146': 60, '1147': 52, '1148': 31, '1149': 94, '1150': 35, '1151': 63, '1152': 74, '1153': 63, '1154': 38, '1155': 60, '1188': 68, '1189': 58, '1190': 63, '1191': 50, '1192': 64, '1193': 44, '1195': 51, '1199': 62, '1202': 31, '1238': 64, '1240': 33, '1241': 25, '1242': 24, '1243': 35, '1244': 21, '1245': 25, '1246': 24, '1247': 32, '1248': 22, '1249': 192, '1250': 37, '1251': 19, '1252': 21, '1253': 30, '1254': 21, '1255': 9, '1262': 31, '1263': 17, '1264': 44, '1265': 20, '1266': 31, '1267': 26, '1268': 31, '1269': 15, '1270': 41, '1271': 34, '1272': 41, '1273': 36, '1274': 56, '1275': 0, '1276': 69, '1277': 73, '1278': 19, '1279': 26, '1280': 21, '1281': 36, '1282': 20, '1283': 50, '1284': 8, '1285': 52, '1286': 30, '1287': 55, '1288': 17, '1289': 47, '1290': 25, '1291': 43, '1292': 25, '1293': 37, '1294': 29, '1295': 42, '1296': 20, '1297': 48, '1298': 34, '1299': 43, '1300': 27, '1301': 53, '1302': 17, '1303': 46, '1304': 18, '1305': 38, '1306': 22, '1307': 20, '1308': 39, '1309': 30, '1310': 18, '1311': 58, '1312': 26, '1313': 23, '1314': 40, '1315': 22, '1316': 24, '1317': 39, '1318': 29, '1319': 16, '1320': 45, '1321': 24, '1322': 38, '1323': 24, '1324': 49, '1325': 89, '1326': 47, '1327': 24, '1328': 37, '1329': 23, '1330': 45, '1331': 22, '1332': 54, '1333': 26, '1334': 58, '1335': 24, '1336': 43, '1337': 26, '1338': 22, '1339': 29, '1340': 90, '1341': 34, '1342': 34, '1343': 29, '1344': 100, '1345': 36, '1346': 24, '1347': 37, '1348': 21, '1349': 46, '1350': 20, '1351': 109, '1352': 20, '1353': 59, '1354': 25, '1355': 39, '1356': 31, '1357': 95, '1358': 26, '1359': 36, '1360': 31, '1361': 30, '1362': 22, '1363': 21, '1364': 31, '1365': 20, '1366': 24, '1367': 44, '1368': 22, '1369': 20, '1370': 28, '1371': 30, '1372': 25, '1373': 182, '1374': 22, '1375': 19, '1376': 33, '1377': 80, '1378': 29, '0381': 23, '1072': 15}\n"
     ]
    }
   ],
   "source": [
    "print(grasp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "goals = np.array(list(grasp_dict.values()))\n",
    "print(goals.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001', '0003', '0006', '0007', '0008', '0009', '0010', '0011', '0012', '0013', '0014', '0015', '0016', '0017', '0019', '0020', '0023', '0024', '0026', '0028', '0031', '0032', '0035', '0037', '0038', '0040', '0042', '0044', '0047', '0048', '0051', '0052', '0055', '0056', '0058', '0061', '0062', '0065', '0067', '0068', '0071', '0072', '0075', '0076', '0079', '0080', '0083', '0084', '0087', '0088', '0091', '0092', '0094', '0096', '0099', '0101', '0102', '0103', '0105', '0106', '0107', '0109', '0111', '0112', '0113', '0114', '0115', '0118', '0119', '0121', '0124', '0127', '0130', '0133', '0140', '0143', '0145', '0146', '0147', '0148', '0149', '0150', '0155', '0156', '0160', '0161', '0162', '0163', '0164', '0168', '0169', '0170', '0171', '0172', '0176', '0177', '0178', '0179', '0181', '0183', '0186', '0187', '0189', '0191', '0192', '0194', '0195', '0196', '0197', '0198', '0200', '0201', '0203', '0204', '0205', '0206', '0207', '0209', '0210', '0211', '0214', '0215', '0219', '0220', '0221', '0227', '0228', '0229', '0230', '0231', '0232', '0233', '0236', '0238', '0240', '0242', '0243', '0244', '0245', '0246', '0248', '0250', '0251', '0254', '0255', '0257', '0260', '0261', '0264', '0266', '0269', '0271', '0272', '0274', '0277', '0278', '0279', '0282', '0286', '0287', '0290', '0293', '0294', '0296', '0301', '0302', '0303', '0304', '0311', '0322', '0324', '0327', '0328', '0330', '0331', '0333', '0335', '0336', '0343', '0344', '0346', '0348', '0349', '0351', '0352', '0353', '0354', '0356', '0360', '0364', '0365', '0366', '0367', '0368', '0370', '0371', '0372', '0374', '0376', '0378', '0379', '0382', '0383', '0386', '0387', '0389', '0390', '0392', '0395', '0396', '0397', '0399', '0400', '0401', '0402', '0404', '0405', '0406', '0407', '0408', '0410', '0412', '0413', '0414', '0416', '0417', '0419', '0420', '0421', '0422', '0425', '0427', '0428', '0430', '0432', '0433', '0434', '0435', '0436', '0437', '0439', '0440', '0443', '0448', '0449', '0450', '0451', '0452', '0455', '0457', '0458', '0459', '0460', '0461', '0462', '0463', '0468', '0469', '0470', '0483', '0487', '0489', '0490', '0492', '0493', '0494', '0496', '0497', '0499', '0502', '0503', '0505', '0506', '0508', '0510', '0511', '0512', '0513', '0515', '0519', '0522', '0525', '0528', '0530', '0531', '0532', '0533', '0535', '0536', '0538', '0539', '0542', '0543', '0546', '0547', '0548', '0552', '0553', '0554', '0557', '0558', '0560', '0561', '0564', '0565', '0566', '0567', '0569', '0570', '0572', '0573', '0574', '0586', '0587', '0588', '0589', '0590', '0591', '0592', '0593', '0594', '0595', '0596', '0600', '0601', '0602', '0603', '0604', '0606', '0607', '0608', '0609', '0610', '0611', '0612', '0613', '0614', '0615', '0616', '0617', '0618', '0619', '0620', '0621', '0622', '0623', '0624', '0625', '0626', '0627', '0628', '0629', '0630', '0631', '0632', '0633', '0634', '0635', '0636', '0637', '0638', '0639', '0640', '0641', '0642', '0643', '0644', '0645', '0646', '0647', '0648', '0649', '0650', '0651', '0652', '0653', '0656', '0657', '0658', '0659', '0660', '0661', '0662', '0663', '0664', '0665', '0666', '0667', '0668', '0669', '0670', '0671', '0672', '0673', '0674', '0675', '0676', '0677', '0678', '0679', '0681', '0682', '0683', '0685', '0686', '0688', '0689', '0690', '0691', '0692', '0697', '0698', '0703', '0716', '0718', '0722', '0726', '0727', '0728', '0732', '0733', '0734', '0736', '0738', '0743', '0745', '0746', '0748', '0749', '0751', '0753', '0754', '0759', '0761', '0763', '0767', '0768', '0769', '0770', '0771', '0773', '0775', '0776', '0777', '0778', '0779', '0780', '0782', '0783', '0784', '0785', '0786', '0787', '0788', '0790', '0792', '0794', '0796', '0797', '0798', '0799', '0800', '0801', '0802', '0803', '0804', '0805', '0806', '0807', '0808', '0809', '0810', '0813', '0814', '0815', '0816', '0818', '0820', '0821', '0823', '0825', '0827', '0829', '0831', '0832', '0833', '0834', '0835', '0836', '0837', '0839', '0843', '0844', '0845', '0846', '0847', '0849', '0850', '0852', '0854', '0856', '0857', '0858', '0859', '0860', '0862', '0863', '0865', '0867', '0868', '0871', '0872', '0876', '0877', '0879', '0882', '0885', '0886', '0887', '0889', '0890', '0891', '0892', '0893', '0894', '0895', '0896', '0902', '0903', '0904', '0905', '0910', '0911', '0915', '0918', '0919', '0920', '0922', '0924', '0926', '0928', '0929', '0931', '0932', '0933', '0935', '0936', '0937', '0938', '0939', '0940', '0941', '0942', '0943', '0944', '0945', '0946', '0947', '0948', '0949', '0950', '0951', '0952', '0955', '0957', '0959', '0961', '0962', '0965', '0966', '0969', '0970', '0973', '0974', '0976', '0978', '0981', '0982', '0983', '0984', '0985', '0986', '0987', '0988', '0989', '0990', '0991', '0992', '0993', '0994', '0995', '0996', '0997', '0998', '0999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1009', '1010', '1011', '1013', '1015', '1016', '1018', '1019', '1020', '1021', '1024', '1025', '1026', '1027', '1028', '1029', '1030', '1031', '1033', '1036', '1038', '1039', '1041', '1044', '1046', '1047', '1048', '1050', '1052', '1054', '1055', '1056', '1057', '1058', '1059', '1061', '1062', '1084', '1085', '1086', '1087', '1132', '1133', '1134', '1135', '1136', '1137', '1140', '1141', '1142', '1143', '1144', '1145', '1146', '1147', '1148', '1149', '1150', '1151', '1152', '1153', '1154', '1155', '1188', '1189', '1190', '1191', '1192', '1193', '1195', '1199', '1202', '1238', '1240', '1241', '1242', '1243', '1244', '1245', '1246', '1247', '1248', '1249', '1250', '1251', '1252', '1253', '1254', '1255', '1256', '1257', '1258', '1259', '1261', '1262', '1263', '1264', '1265', '1266', '1267', '1268', '1269', '1270', '1271', '1272', '1273', '1274', '1275', '1276', '1277', '1278', '1279', '1280', '1281', '1282', '1283', '1284', '1285', '1286', '1287', '1288', '1289', '1290', '1291', '1292', '1293', '1294', '1295', '1296', '1297', '1298', '1299', '1300', '1301', '1302', '1303', '1304', '1305', '1306', '1307', '1308', '1309', '1310', '1311', '1312', '1313', '1314', '1315', '1316', '1317', '1318', '1319', '1320', '1321', '1322', '1323', '1324', '1325', '1326', '1327', '1328', '1329', '1330', '1331', '1332', '1333', '1334', '1335', '1336', '1337', '1338', '1339', '1340', '1341', '1342', '1343', '1344', '1345', '1346', '1347', '1348', '1349', '1350', '1351', '1352', '1353', '1354', '1355', '1356', '1357', '1358', '1359', '1360', '1361', '1362', '1363', '1364', '1365', '1366', '1367', '1368', '1369', '1370', '1371', '1372', '1373', '1374', '1375', '1376', '1377', '1378', '0381', '1072']\n"
     ]
    }
   ],
   "source": [
    "goals_seq = list(grasp_dict.keys())\n",
    "print(goals_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_002 = []\n",
    "for seq in goals_seq:\n",
    "    seq_path = join(datapath,seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "        hand_trans = hand_params[:,:3]\n",
    "        hand_theta = hand_params[:,3:51]\n",
    "        mano_beta = hand_params[:,51:]\n",
    "        active_obj = meta['active_obj']\n",
    "        # print(active_obj)\n",
    "        if active_obj.startswith('002_qua') or active_obj.startswith('002_tri'):\n",
    "            seq_002.append(seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_002\n",
    "for seq in seq_002:\n",
    "    del grasp_dict[seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去掉失败的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq,index in grasp_dict.items():\n",
    "    seq_path = join(datapath,seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    # print(meta)\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        hand_params = np.load(join(seq_path,'mano/poses_right.npy'))\n",
    "    #     meta['goal_index'] = index\n",
    "    # with open(meta_path,'wb')as f:\n",
    "    #     pickle.dump(meta, f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_pose = hand_params[:5,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs = list(grasp_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = '/root/code/seqs/gazehoi_list_train_new.txt'\n",
    "testpath = '/root/code/seqs/gazehoi_list_test_new.txt'\n",
    "        \n",
    "with open(trainpath,'r') as f:\n",
    "    train_list = f.readlines()\n",
    "with open(testpath,'r') as f:\n",
    "    test_list = f.readlines()\n",
    "\n",
    "valid_seqs = []\n",
    "for info in train_list:\n",
    "    seq = info.strip()\n",
    "    valid_seqs.append(seq)\n",
    "for info in test_list:\n",
    "    seq = info.strip()\n",
    "    valid_seqs.append(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 684)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs),len(new_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"/root/code/seqs/gazehoi_list_train_new.txt\",\"w\")\n",
    " \n",
    "# f.writelines(test)\n",
    "for line in new_seqs:\n",
    "    f.write(line+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63,)\n"
     ]
    }
   ],
   "source": [
    "path1 = '/root/code/OmniControl/dataset/kit_spatial_norm/Mean_raw.npy'\n",
    "print(np.load(path1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251,)\n"
     ]
    }
   ],
   "source": [
    "path1 = '/root/code/OmniControl/dataset/kit_mean.npy' \n",
    "print(np.load(path1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:13<00:00, 55.88it/s] \n"
     ]
    }
   ],
   "source": [
    "# seqs = sorted(os.listdir(valid_seqs))\n",
    "from tqdm import *\n",
    "hands_list = []\n",
    "joint_list = []\n",
    "for seq in tqdm(valid_seqs):\n",
    "    seq_path = join('/root/code/seqs/1205_data/',seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "        hand_trans = hand_params[:,:3]\n",
    "        hand_rot = hand_params[:,3:6]\n",
    "        hand_theta = hand_params[:,3:51]\n",
    "        mano_beta = hand_params[:,51:]\n",
    "\n",
    "        gt_output = manolayer(hand_theta, mano_beta)\n",
    "        # 相对表示\n",
    "        gt_joints = gt_output.joints - gt_output.joints[:, 0].unsqueeze(1) + hand_trans.unsqueeze(1)\n",
    "        joint_list.append(gt_joints)\n",
    "        # hands_list.append(hand_params)\n",
    "        # hand_trans = hand_params[:,:3]\n",
    "        # hand_theta = hand_params[:,3:51]\n",
    "        # mano_beta = hand_params[:,51:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:03<00:00, 243.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# seqs = sorted(os.listdir(valid_seqs))\n",
    "from tqdm import *\n",
    "from utils.data_util import obj_global2local_matrix, obj_matrix2rot6d\n",
    "hands_list = []\n",
    "joint_list = []\n",
    "local_obj_pose_6d_list = []\n",
    "global_obj_pose_6d_list = []\n",
    "for seq in tqdm(valid_seqs):\n",
    "    seq_path = join('/root/code/seqs/1205_data/',seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        obj_name_list = meta['obj_name_list']\n",
    "        for obj in obj_name_list:\n",
    "            obj_pose = torch.tensor(np.load(join(seq_path,obj+'_pose_trans.npy')).reshape(-1,3,4))\n",
    "            obj_pose_local = obj_global2local_matrix(obj_pose.unsqueeze(0))\n",
    "            obj_pose_local_6d = obj_matrix2rot6d(obj_pose_local)\n",
    "            obj_pose_global_6d = obj_matrix2rot6d(obj_pose.unsqueeze(0))\n",
    "            local_obj_pose_6d_list.append(obj_pose_local_6d.squeeze(0))\n",
    "            global_obj_pose_6d_list.append(obj_pose_global_6d.squeeze(0))\n",
    "    \n",
    "        \n",
    "        # hands_list.append(hand_params)\n",
    "        # hand_trans = hand_params[:,:3]\n",
    "        # hand_theta = hand_params[:,3:51]\n",
    "        # mano_beta = hand_params[:,51:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data = np.concatenate(local_obj_pose_6d_list, axis=0)\n",
    "global_data = np.concatenate(global_obj_pose_6d_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((474789, 9), (474789, 9))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data.shape, global_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79933405 -0.72934205 -1.19342614 -0.21386048  0.00379753  0.17496449\n",
      "  0.09812263 -0.05653823 -0.04264879] [0.13808932 0.13808932 0.13808932 0.56078171 0.56078171 0.56078171\n",
      " 0.56078171 0.56078171 0.56078171]\n"
     ]
    }
   ],
   "source": [
    "# hands_pose = data[:,:51]\n",
    "# print(hands_pose.shape)\n",
    "data = global_data\n",
    "mean = data.mean(axis=0)\n",
    "std = data.std(axis=0)\n",
    "\n",
    "std[:3] = std[:3].mean() / 1.0\n",
    "std[3:] = std[3:].mean() / 1.0\n",
    "\n",
    "print(mean,std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/code/OmniControl/dataset/gazehoi_global_obj_mean.npy', mean)\n",
    "np.save('/root/code/OmniControl/dataset/gazehoi_global_obj_std.npy', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.65283066, -0.87845784, -1.3190148 ],\n",
       "        [ 0.6552333 , -0.84926456, -1.3022171 ],\n",
       "        [ 0.66498834, -0.83184993, -1.2834679 ],\n",
       "        [ 0.68043625, -0.82334834, -1.267413  ],\n",
       "        [ 0.69987905, -0.81139255, -1.2479547 ],\n",
       "        [ 0.65505785, -0.84640986, -1.2444972 ],\n",
       "        [ 0.67369163, -0.8385318 , -1.2257823 ],\n",
       "        [ 0.6897996 , -0.83293426, -1.2187285 ],\n",
       "        [ 0.7052852 , -0.82371163, -1.2139604 ],\n",
       "        [ 0.663287  , -0.8641404 , -1.2366261 ],\n",
       "        [ 0.68344283, -0.85599595, -1.2219536 ],\n",
       "        [ 0.69989264, -0.8476628 , -1.2189465 ],\n",
       "        [ 0.7173478 , -0.8390383 , -1.2199993 ],\n",
       "        [ 0.6740651 , -0.88206345, -1.2463697 ],\n",
       "        [ 0.68853736, -0.8710868 , -1.2313952 ],\n",
       "        [ 0.7053893 , -0.86183923, -1.2284079 ],\n",
       "        [ 0.7201399 , -0.8533746 , -1.2264625 ],\n",
       "        [ 0.6846773 , -0.89362437, -1.2567991 ],\n",
       "        [ 0.698029  , -0.88793355, -1.2473216 ],\n",
       "        [ 0.70955783, -0.8794469 , -1.2435588 ],\n",
       "        [ 0.7200982 , -0.8713427 , -1.2403091 ]], dtype=float32),\n",
       " (21, 3))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean,std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "std[:3] = std[:3].mean() / 1.0\n",
    "std[3:6] = std[3:6].mean() / 1.0\n",
    "std[6:] = std[6:].mean() / 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/code/OmniControl/dataset/gazehoi_global_joint_mean.npy', mean)\n",
    "np.save('/root/code/OmniControl/dataset/gazehoi_global_joint_std.npy', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load('/root/code/OmniControl/save/my_omnicontrol2/samples_my_omnicontrol2_000050000_seed10_predefined/results.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['motion', 'lengths', 'hint', 'seqs', 'num_samples', 'num_repetitions'])\n"
     ]
    }
   ],
   "source": [
    "print(res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(res['seqs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_eval(x, hint, n_joints=22):\n",
    "    # mask = hint != 0\n",
    "    # hint = hint * raw_std + raw_mean\n",
    "    # hint = hint * mask\n",
    "    # hint = hint.transpose((0, 2, 1))\n",
    "    # print(hint.shape,motion.shape)\n",
    "    # # hint = hint.reshape(hint.shape[0], , -1)\n",
    "    # mask = hint.sum(axis=1, keepdims=True) != 0\n",
    "    # print(mask.shape)\n",
    "    # loss = np.linalg.norm((motion - hint) * mask, axis=1)\n",
    "    # loss = loss.sum() / mask.sum()\n",
    "    mask_hint = hint.view(hint.shape[0], hint.shape[1], 17, 3).sum(dim=-1, keepdim=True) != 0\n",
    "\n",
    "    x_ = x.permute(0, 3, 2, 1).contiguous()\n",
    "    x_ = x_.squeeze(2)\n",
    "    bs,nf,_ = x_.shape\n",
    "    print(hint.shape,x.shape,mask_hint.shape)\n",
    "    loss = torch.norm((x_.reshape(bs,nf,-1,3) - hint.reshape(bs,nf,-1,3)) * mask_hint, dim=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 60, 51]) torch.Size([81, 51, 1, 60]) torch.Size([81, 60, 17, 1])\n",
      "Guide:  tensor(0.0207, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# from utils.simple_eval import simple_eval\n",
    "all_motions = torch.tensor(res['motion'])\n",
    "all_hint = torch.tensor(res['hint'])\n",
    "loss = simple_eval(all_motions, all_hint)\n",
    "# print(loss)\n",
    "print(\"Guide: \",torch.mean(loss[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 60, 51)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hint.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 60, 51])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([81, 60, 51])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "path = '/root/code/OmniControl/save/guide_delay2/samples_guide_delay2_000030000_seed10_predefined/results.npy'\n",
    "res = np.load(path,allow_pickle=True).item()\n",
    "pred_motion = torch.tensor(res['motion']).permute(0, 3, 2, 1).squeeze(2).contiguous()\n",
    "hint = torch.tensor(res['hint'])\n",
    "print(hint.shape)\n",
    "seqs = res['seqs']\n",
    "pred_motion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import rotation_6d_to_matrix,axis_angle_to_matrix\n",
    "\n",
    "res_save_path = path.replace('npy','txt')\n",
    "print(res_save_path)\n",
    "stage1_eval(pred_motion, hint, seqs, res_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_goal_err(x, hint):\n",
    "    \"\"\"\n",
    "    每个joints的平均偏差?\n",
    "    \"\"\"\n",
    "    mask_hint = hint.view(hint.shape[0], hint.shape[1], 17, 3).sum(dim=-1, keepdim=True) != 0\n",
    "    x_ = x\n",
    "    print(x.shape)\n",
    "    bs,nf,_ = x_.shape\n",
    "    loss = torch.norm((x_.reshape(bs,nf,-1,3) - hint.reshape(bs,nf,-1,3)) * mask_hint, dim=-1)\n",
    "    # print(loss[:,0])\n",
    "    loss = loss[:,0] # bs,17\n",
    "    # loss = torch.sum(loss) / bs\n",
    "    loss = torch.mean(loss[:,0])\n",
    "    return loss\n",
    "\n",
    "def stage1_eval(pred_motion, all_hint, all_seqs, res_save_path):\n",
    "    pred_motion = torch.tensor(pred_motion)\n",
    "    hint = torch.tensor(all_hint)\n",
    "    goal_err = cal_goal_err(pred_motion,hint)\n",
    "\n",
    "    num_samples = pred_motion.shape[0]\n",
    "\n",
    "\n",
    "    datapath = '/root/code/seqs/1205_data/'\n",
    "    manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano',side='right')\n",
    "    obj_path = '/root/code/seqs/object/'\n",
    "\n",
    "    total_hand_T_error = 0\n",
    "    total_hand_R_error = 0\n",
    "    total_mpjpe = 0 # root-relative\n",
    "    total_traj_invalid_10 = 0\n",
    "    total_traj_invalid_20 = 0\n",
    "    total_traj_num = 0\n",
    "    total_hand_R_error = 0\n",
    "\n",
    "    for i in range(pred_motion.shape[0]):\n",
    "        seq = seqs[i]\n",
    "        seq_path = join(datapath,seq)\n",
    "\n",
    "        meta_path = join(seq_path,'meta.pkl')\n",
    "        with open(meta_path,'rb')as f:\n",
    "            meta = pickle.load(f)\n",
    "            \n",
    "        active_obj = meta['active_obj']\n",
    "        obj_pose = np.load(join(seq_path,active_obj+'_pose_trans.npy'))\n",
    "        \n",
    "        goal_index = meta['goal_index']\n",
    "        obj_pose = torch.tensor(obj_pose[:goal_index+1]).float()\n",
    "\n",
    "        if goal_index < 59:\n",
    "            hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))[:goal_index+1]\n",
    "        else:\n",
    "            hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))[goal_index-59:goal_index+1]\n",
    "\n",
    "        hand_trans = hand_params[:,:3]\n",
    "        hand_rot = hand_params[:,3:6]\n",
    "        hand_theta = hand_params[:,3:51]\n",
    "        mano_beta = hand_params[:,51:]\n",
    "\n",
    "        pred_trans = torch.flip(pred_motion[i,:goal_index+1,:3],dims=[0])\n",
    "        pred_theta = torch.flip(pred_motion[i,:goal_index+1,3:],dims=[0])\n",
    "        pred_rot = torch.flip(pred_motion[i,:goal_index+1,3:6],dims=[0])\n",
    "\n",
    "        pred_output = manolayer(pred_theta, mano_beta)\n",
    "        # 相对表示\n",
    "        pred_joints = pred_output.joints - pred_output.joints[:, 0].unsqueeze(1)\n",
    "        gt_output = manolayer(hand_theta, mano_beta)\n",
    "        # 相对表示\n",
    "        gt_joints = gt_output.joints - gt_output.joints[:, 0].unsqueeze(1)\n",
    "        mpjpe = torch.sum(torch.norm(pred_joints - gt_joints, dim=-1)) / (hand_trans.shape[0] * 21)\n",
    "        total_mpjpe += mpjpe\n",
    "\n",
    "        hand_T_error = torch.sum(torch.norm(hand_trans-pred_trans,p=2,dim=-1)) / hand_trans.shape[0]\n",
    "        total_hand_T_error += hand_T_error\n",
    "\n",
    "        hand_rot = axis_angle_to_matrix(hand_rot)\n",
    "        pred_rot = axis_angle_to_matrix(pred_rot)\n",
    "        hand_rot = torch.einsum('...ij->...ji', [hand_rot])\n",
    "        hand_R_error = (torch.einsum('fpn,fnk->fpk',hand_rot,pred_rot) - torch.eye(3).unsqueeze(0).repeat(hand_rot.shape[0],1,1)).reshape(-1,9) # nf,3,3\n",
    "        hand_R_error =  torch.sum(torch.norm(hand_R_error,dim=-1)) /hand_trans.shape[0]\n",
    "        total_hand_R_error += hand_R_error\n",
    "\n",
    "        traj_error = torch.norm(hand_trans-pred_trans,p=2,dim=1) \n",
    "        traj_invalid_10 = torch.sum(traj_error > 0.1)\n",
    "        traj_invalid_20 = torch.sum(traj_error > 0.2)\n",
    "        total_traj_invalid_10 += traj_invalid_10\n",
    "        total_traj_invalid_20 += traj_invalid_20\n",
    "        total_traj_num += traj_error.shape[0]\n",
    "    \n",
    "    with open(res_save_path, 'w') as f:\n",
    "        f.write(f'Goal Error:{goal_err:.6f}\\n')\n",
    "        f.write(f'Traj Error (<10cm):{(total_traj_invalid_10/total_traj_num):.6f}\\n')\n",
    "        f.write(f'Traj Error (<20cm):{(total_traj_invalid_20/total_traj_num):.6f}\\n')\n",
    "        f.write(f'Hand Trans Error:{total_hand_T_error/num_samples:.6f}\\n')\n",
    "        f.write(f'Hand Rot Error:{total_hand_R_error/num_samples:.6f}\\n')\n",
    "        f.write(f'MPJPE :{total_mpjpe/num_samples:.6f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_motion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_473270/822654735.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalib_dome\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_motion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mseq_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_motion' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from manotorch.manolayer import ManoLayer\n",
    "from os.path import join\n",
    "import json\n",
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'osmesa'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "def vis_smpl(out_path, image, nf, vertices, faces, camera_R, camera_T, K = np.array([1031.8450927734375, 0.0, 932.1596069335938, 0.0, 1022.4588623046875, 541.9437255859375, 0.0, 0.0, 1.0]).reshape((3,3))):\n",
    "    outname = os.path.join(out_path, '{:06d}.jpg'.format(nf))\n",
    "    # if os.path.exists(outname): return\n",
    "    render_data = {}\n",
    "    assert vertices.shape[1] == 3 and len(vertices.shape) == 2, 'shape {} != (N, 3)'.format(vertices.shape)\n",
    "    render_data = {\"vertices\": vertices, \"faces\": faces, \"vid\":0, \"name\": \"human_{}_0\".format(nf)}\n",
    "\n",
    "    camera = {\"K\": K,\n",
    "        \"R\": camera_R,\n",
    "        \"T\":camera_T}\n",
    "    from visualize.renderer import Renderer\n",
    "    render = Renderer(height=3840, width=2160, faces=None)\n",
    "    image_vis, depth = render.render(render_data, camera, image, add_back=True)\n",
    "    # print(depth)\n",
    "    cv2.imwrite(outname, image_vis)\n",
    "    return image_vis, depth  \n",
    "\n",
    "datapath = '/root/code/seqs/1205_data/'\n",
    "manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano',side='right')\n",
    "faces = manolayer.th_faces\n",
    "view = 5\n",
    "calib_path = \"/root/code/seqs/calibration_all.json\"\n",
    "with open(calib_path) as f:\n",
    "    calib_dome = json.load(f)\n",
    "    f.close()\n",
    "camera_pose = np.vstack((np.asarray(calib_dome[str(view)]['RT']).reshape((3,4)), np.ones(4) ))\n",
    "K = np.asarray(calib_dome[str(view)]['K']).reshape((3,3))\n",
    "\n",
    "for i in range(pred_motion.shape[0]):\n",
    "    seq = seqs[i]\n",
    "    seq_path = join(datapath,seq)\n",
    "    hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "    hand_trans = hand_params[:,:3]\n",
    "    hand_theta = hand_params[:,3:51]\n",
    "    mano_beta = hand_params[:,51:]\n",
    "\n",
    "    pred_trans = pred_motion[i,:,:3]\n",
    "    pred_theta = pred_motion[i,:,3:]\n",
    "    mano_output = manolayer(pred_theta, mano_beta[:goal+1])\n",
    "    print(mano_output.verts.shape,mano_output.joints.shape,pred_trans.shape)\n",
    "    mano_verts = mano_output.verts - mano_output.joints[:, 0].unsqueeze(1) + pred_trans.unsqueeze(1)\n",
    "    render_out = join('/root/code/OmniControl/save/guide_delay2/samples_guide_delay2_000030000_seed10_predefined/','render')\n",
    "    os.makedirs(render_out,exist_ok=True)\n",
    "    for k in range(len(mano_verts)):\n",
    "        img = np.zeros((2160, 3840,3), np.uint8) \n",
    "        \n",
    "        _, depth = vis_smpl(render_out, img, k, mano_verts[k], faces,camera_pose[:3,:3],camera_pose[:3,3],K)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.90727966  0.46295225 -0.35474287  1.6237592  -0.29448975 -1.81806888\n",
      "   0.26575419  1.02130978 -1.11818234  2.30116171]]\n",
      "[[-0.90727966  0.46295225 -0.35474287  1.6237592  -0.29448975 -1.81806888\n",
      "   0.26575419  1.02130978 -1.11818234  2.30116171]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(1,10)\n",
    "b = a[:,-10:]\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengths_to_mask(lengths, max_len):\n",
    "    # max_len = max(lengths)\n",
    "    mask = torch.arange(max_len-1,-1,-1).expand(len(lengths), max_len) < lengths.unsqueeze(1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 4, 3, 2, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True],\n",
       "         [False,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True],\n",
       "         [ True,  True,  True]]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = torch.tensor([[2],[3]])\n",
    "print(lens.shape)\n",
    "lengths_to_mask(lens,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 61])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 175, 51])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "import torch\n",
    "from pytorch3d.transforms import  quaternion_to_axis_angle,axis_angle_to_quaternion, axis_angle_to_matrix, matrix_to_rotation_6d,rotation_6d_to_matrix,matrix_to_axis_angle\n",
    "from data_loaders.humanml.common.quaternion import qinv, qmul\n",
    "seq_path = '/root/code/seqs/1205_data/0001/'\n",
    "hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "hand_motion = hand_params[:,:51]\n",
    "hand_motion = (hand_motion.unsqueeze(0).repeat(10,1,1)).contiguous()\n",
    "print(hand_motion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 175, 51])\n",
      "torch.Size([10, 175, 99])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2167e+00, -3.4285e+00,\n",
      "        -4.1685e+00, -7.4506e-09,  1.4901e-08, -5.9605e-08, -1.8626e-09,\n",
      "         3.5482e-10, -5.9605e-08, -9.3132e-10,  8.7587e-11, -2.9802e-08,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.4901e-08, -1.9550e-09,\n",
      "         0.0000e+00,  0.0000e+00,  2.3411e-10,  0.0000e+00, -2.9802e-08,\n",
      "        -1.4901e-08,  0.0000e+00,  1.4901e-08,  0.0000e+00,  0.0000e+00,\n",
      "         1.4901e-08, -2.3381e-10, -1.4901e-08,  7.4506e-09, -4.4703e-08,\n",
      "        -5.9605e-08, -7.4506e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  5.9605e-08, -5.9605e-08,  2.9802e-08,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00])\n",
      "tensor(-0.0023)\n"
     ]
    }
   ],
   "source": [
    "# from utils.data_util import  global2local_axis,local2global_axis\n",
    "\n",
    "raw = hand_motion\n",
    "# local = global2local_axis(raw)\n",
    "# after_axis = local2global_axis(local)\n",
    "print(raw.shape)\n",
    "after_rot = axis2rot6d(raw)\n",
    "print(after_rot.shape)\n",
    "axis = rot6d2axis(after_rot)\n",
    "print((after_axis-axis)[0,2])\n",
    "print(torch.sum(after_axis-axis)/(1750*51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.3535e-01,  4.3242e-01, -4.6088e-01, -5.6036e-01],\n",
      "         [ 9.5530e-01,  4.3895e-02, -9.3296e-02,  2.7707e-01],\n",
      "         [ 9.8818e-01,  9.1567e-03,  0.0000e+00,  1.5302e-01],\n",
      "         [ 9.9764e-01,  4.1005e-03,  0.0000e+00,  6.8522e-02],\n",
      "         [ 9.6868e-01, -4.8997e-02,  2.3991e-02,  2.4225e-01],\n",
      "         [ 9.6427e-01, -4.6077e-02,  0.0000e+00,  2.6090e-01],\n",
      "         [ 9.9590e-01, -1.5728e-02, -3.7406e-09,  8.9057e-02],\n",
      "         [ 9.5400e-01, -1.8771e-01,  1.1988e-01,  2.0069e-01],\n",
      "         [ 9.9200e-01, -7.4046e-02,  0.0000e+00,  1.0220e-01],\n",
      "         [ 9.9686e-01, -4.6456e-02,  7.4740e-09,  6.4119e-02],\n",
      "         [ 9.7246e-01, -5.9505e-02,  1.2219e-01,  1.8935e-01],\n",
      "         [ 9.5948e-01, -5.0303e-02,  0.0000e+00,  2.7725e-01],\n",
      "         [ 9.9440e-01, -1.8873e-02,  0.0000e+00,  1.0402e-01],\n",
      "         [ 9.0321e-01,  3.4466e-01, -1.8801e-01,  1.7339e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n",
      "tensor([[[-5.3535e-01, -4.3242e-01,  4.6088e-01,  5.6036e-01],\n",
      "         [ 9.5530e-01,  4.3895e-02, -9.3296e-02,  2.7707e-01],\n",
      "         [ 9.8818e-01,  9.1567e-03, -1.7671e-10,  1.5302e-01],\n",
      "         [ 9.9764e-01,  4.1005e-03, -4.3759e-11,  6.8522e-02],\n",
      "         [ 9.6868e-01, -4.8997e-02,  2.3991e-02,  2.4225e-01],\n",
      "         [ 9.6427e-01, -4.6077e-02,  9.6584e-10,  2.6090e-01],\n",
      "         [ 9.9590e-01, -1.5728e-02, -3.8575e-09,  8.9057e-02],\n",
      "         [ 9.5400e-01, -1.8771e-01,  1.1988e-01,  2.0069e-01],\n",
      "         [ 9.9200e-01, -7.4046e-02,  0.0000e+00,  1.0220e-01],\n",
      "         [ 9.9686e-01, -4.6456e-02,  7.5908e-09,  6.4119e-02],\n",
      "         [ 9.7246e-01, -5.9505e-02,  1.2219e-01,  1.8935e-01],\n",
      "         [ 9.5948e-01, -5.0303e-02,  0.0000e+00,  2.7725e-01],\n",
      "         [ 9.9440e-01, -1.8873e-02,  0.0000e+00,  1.0402e-01],\n",
      "         [ 9.0321e-01,  3.4466e-01, -1.8801e-01,  1.7339e-01],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "theta = raw[0,2,3:].reshape(-1,3).unsqueeze(0)\n",
    "# print(theta)\n",
    "rot6d = matrix_to_rotation_6d(axis_angle_to_matrix(theta))\n",
    "theta_after = matrix_to_axis_angle(rotation_6d_to_matrix(rot6d))\n",
    "diff = theta_after - theta\n",
    "# print(diff)\n",
    "# print(abs(diff) > 1e-6)\n",
    "print(axis_angle_to_quaternion(theta))\n",
    "print(axis_angle_to_quaternion(theta_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch3d.transforms import  quaternion_to_axis_angle,axis_angle_to_quaternion, axis_angle_to_matrix, matrix_to_rotation_6d,rotation_6d_to_matrix,matrix_to_axis_angle\n",
    "from data_loaders.humanml.common.quaternion import qinv, qmul\n",
    "def axis2rot6d(motion):\n",
    "    # motion (bs, frames, 51)\n",
    "    bs, nf, _ = motion.shape\n",
    "    motion_rot_axis = motion[:,:,3:].reshape(bs,nf,-1,3)\n",
    "    motion_rot_6d = matrix_to_rotation_6d(axis_angle_to_matrix(motion_rot_axis))\n",
    "    motion_rot_6d = motion_rot_6d.reshape(bs, nf, -1)\n",
    "    motion = torch.cat((motion[:,:,:3], motion_rot_6d),dim=-1)\n",
    "    return motion\n",
    "\n",
    "def rot6d2axis(motion):\n",
    "    # motion (bs, frames, 99)\n",
    "    bs, nf, _ = motion.shape\n",
    "    motion_rot_6d = motion[:,:,3:].reshape(bs,nf,-1,6)\n",
    "    motion_axis = matrix_to_axis_angle(rotation_6d_to_matrix(motion_rot_6d))\n",
    "    motion_axis = motion_axis.reshape(bs, nf, -1)\n",
    "    motion = torch.cat((motion[:,:,:3], motion_axis),dim=-1)\n",
    "    return motion \n",
    "    \n",
    "def get_vel_axis(motion):\n",
    "    # motion (bs, frames, 51)\n",
    "    trans = motion[:,:,:3]\n",
    "    rot_axis = motion[:,:,3:6]\n",
    "    l_velocity = trans[:,1:] - trans[:,:-1] #bs, frames-1 ,3\n",
    "\n",
    "    rot_qua = axis_angle_to_quaternion(rot_axis)\n",
    "    print(rot_qua[:,:-1].shape)\n",
    "    r_velocity = qmul(rot_qua[:,1:], qinv(rot_qua[:,:-1]))\n",
    "    r_velocity = quaternion_to_axis_angle(r_velocity)\n",
    "\n",
    "    return l_velocity, r_velocity\n",
    "\n",
    "def global2local_axis(motion):\n",
    "    # 只对global RT做相对变换\n",
    "    l_velocity, r_velocity = get_vel_axis(motion)\n",
    "    motion[:,1:,:3] = l_velocity\n",
    "    motion[:,1:,3:6] = r_velocity\n",
    "    return motion\n",
    "\n",
    "\n",
    "def local2global_axis(motion):\n",
    "    # 只对global RT做相对变换\n",
    "    l_velocity = motion[:,:,:3]\n",
    "    r_velocity = motion[:,:,3:6]\n",
    "\n",
    "    trans = torch.cumsum(l_velocity,dim=1)\n",
    "\n",
    "    r_velocity = axis_angle_to_quaternion(r_velocity)\n",
    "    nf = l_velocity.shape[1]\n",
    "    rot = r_velocity[:,0].unsqueeze(1)\n",
    "    cur_r = rot\n",
    "    for i in range(1,nf):\n",
    "        rot_i = qmul(r_velocity[:,i].unsqueeze(1), cur_r)\n",
    "        cur_r = rot_i\n",
    "        # print(rot_i.shape)\n",
    "        rot = torch.cat((rot,rot_i),dim=1)\n",
    "    rot = quaternion_to_axis_angle(rot)\n",
    "    motion[:,:,:3] = trans\n",
    "    motion[:,:,3:6] = rot\n",
    "\n",
    "    return motion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vel_axis_by_matrix(motion):\n",
    "    trans = motion[:,:,:3]\n",
    "    rot_axis = motion[:,:,3:6]\n",
    "    l_velocity = trans[:,1:] - trans[:,:-1] #bs, frames-1 ,3\n",
    "\n",
    "    rot_matrix = axis_angle_to_matrix(rot_axis) # bs. frames, 3, 3\n",
    "    matrix_0 = rot_matrix[:,:-1] # 需要求逆\n",
    "    matrix_1 = rot_matrix[:,1:]\n",
    "    matrix_0_inv = torch.einsum('...ij->...ji', [matrix_0])\n",
    "    r_velocity = torch.einsum('fipn,fink->fipk',matrix_1,matrix_0_inv)\n",
    "    r_velocity = matrix_to_axis_angle(r_velocity)\n",
    "    return l_velocity, r_velocity\n",
    "\n",
    "def global2local_axis_by_matrix(motion):\n",
    "    l_velocity, r_velocity = get_vel_axis_by_matrix(motion)\n",
    "    # motion[:,1:,:3] = l_velocity\n",
    "    # motion[:,1:,3:6] = r_velocity\n",
    "    new_motion = motion.clone()\n",
    "    new_motion[:, 1:, :3] = l_velocity\n",
    "    new_motion[:, 1:, 3:6] = r_velocity\n",
    "    return new_motion\n",
    "\n",
    "def local2global_axis_by_matrix(motion):\n",
    "     # 只对global RT做相对变换\n",
    "    l_velocity = motion[:,:,:3]\n",
    "    r_velocity = motion[:,:,3:6]\n",
    "\n",
    "    trans = torch.cumsum(l_velocity,dim=1)\n",
    "\n",
    "    r_velocity = axis_angle_to_matrix(r_velocity)\n",
    "    print(r_velocity.shape)\n",
    "    # matrices = [r for r in r_velocity]\n",
    "    # rot = torch.chain_matmul(*matrices)\n",
    "    nf = l_velocity.shape[1]\n",
    "    rot = r_velocity[:,0].unsqueeze(1) #bs,1,3,3\n",
    "    cur_r = rot\n",
    "    for i in range(1,nf):\n",
    "        # print(r_velocity[:,i].unsqueeze(1).shape, cur_r.shape)\n",
    "        rot_i = torch.einsum('fipn,fink->fipk',r_velocity[:,i].unsqueeze(1),cur_r)\n",
    "        cur_r = rot_i\n",
    "        rot = torch.cat((rot,rot_i),dim=1)\n",
    "\n",
    "    print(rot.shape)\n",
    "    rot = matrix_to_axis_angle(rot)\n",
    "    print(rot.shape)\n",
    "    new_motion = motion.clone()\n",
    "    new_motion[:,:,:3] = trans\n",
    "    new_motion[:,:,3:6] = rot\n",
    "    return new_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 175, 3, 3])\n",
      "torch.Size([10, 175, 3, 3])\n",
      "torch.Size([10, 175, 3])\n"
     ]
    }
   ],
   "source": [
    "local = global2local_axis_by_matrix(hand_motion)\n",
    "global_after = local2global_axis_by_matrix(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 174, 4])\n"
     ]
    }
   ],
   "source": [
    "local = global2local_axis(hand_motion)\n",
    "global_after = local2global_axis(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = '/root/code/seqs/gazehoi_list_train_new.txt'\n",
    "testpath = '/root/code/seqs/gazehoi_list_test_new.txt'\n",
    "        \n",
    "with open(trainpath,'r') as f:\n",
    "    train_list = f.readlines()\n",
    "with open(testpath,'r') as f:\n",
    "    test_list = f.readlines()\n",
    "\n",
    "valid_seqs = []\n",
    "for info in train_list:\n",
    "    seq = info.strip()\n",
    "    valid_seqs.append(seq)\n",
    "for info in test_list:\n",
    "    seq = info.strip()\n",
    "    valid_seqs.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/765 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:00<00:00, 3248.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "motion_list = []\n",
    "hint_list = []\n",
    "lens = []\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "\n",
    "def convert_to_obj_frame(points, obj_pose):\n",
    "    # points(frames,5,3)\n",
    "    # obj_pose (frames,3,4)\n",
    "\n",
    "    # pc = (obj_rot.T @ (pc - obj_trans).T).T\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    # print(obj_T.shape)\n",
    "    points = points - obj_T\n",
    "    # hand_rot = torch.einsum('...ij->...ji', [hand_rot])\n",
    "    # hand_R_error = (torch.einsum('fpn,fnk->fpk',hand_rot,pred_rot) - torch.eye(3).unsqueeze(0).repeat(hand_rot.shape[0],1,1)).reshape(-1,9) # nf,3,3\n",
    "       \n",
    "    points = torch.einsum('...ij->...ji', [points])\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    new_points = torch.einsum('fpn,fnk->fpk',obj_R,points)\n",
    "    new_points = torch.einsum('...ij->...ji', [new_points])\n",
    "\n",
    "    return new_points\n",
    "\n",
    "for seq in tqdm(valid_seqs):\n",
    "    seq_path = join('/root/code/seqs/1205_data/',seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    goal_index = meta['goal_index']\n",
    "    active_obj = meta['active_obj']\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy'))).float()\n",
    "        lens.append(hand_params.shape[0])\n",
    "        \"\"\"\n",
    "        # obj_pose = torch.tensor(np.load(join(seq_path,active_obj+'_pose_trans.npy'))).float()\n",
    "        # # print(hand_params.shape, obj_pose.shape)\n",
    "        # obj_verts = torch.tensor(np.load(join(obj_path,active_obj,'resampled_500_trans.npy'))).float()\n",
    "        # goal_obj_pose = obj_pose[goal_index]\n",
    "        # # obj_verts = obj_verts @ goal_obj_pose[:3,:3].T + goal_obj_pose[:3,3].reshape(1,3)\n",
    "        # kdt = KDTree(obj_verts)\n",
    "        \n",
    "        # hand_trans = hand_params[:,:3]\n",
    "        # hand_rot = hand_params[:,3:6]\n",
    "        # hand_theta = hand_params[:,3:51]\n",
    "        # mano_beta = hand_params[:,51:]\n",
    "\n",
    "        # gt_output = manolayer(hand_theta, mano_beta)\n",
    "        # gt_joints = gt_output.joints - gt_output.joints[:, 0].unsqueeze(1) + hand_trans.unsqueeze(1)\n",
    "\n",
    "        # tips = [15,3,6,12,9]\n",
    "        # tips_T = gt_joints[goal_index:,tips]\n",
    "        # # print(tips_T.shape)\n",
    "        # obj_pose = obj_pose[goal_index:]\n",
    "        # tips_to_obj = convert_to_obj_frame(tips_T, obj_pose) # (frames,5,3) (frames,3,4)\n",
    "        # # print(tips_to_obj.shape)\n",
    "        # ds, ids = kdt.query(tips_to_obj.reshape(-1,3), k=1)\n",
    "        # ids = torch.from_numpy(ids)\n",
    "        # ds = torch.from_numpy(ds)\n",
    "        # # print(ds.shape, ids.shape)\n",
    "        # target_verts = obj_verts[ids.squeeze()].reshape(-1,5,3)\n",
    "        # ids = ids.reshape(-1,5)\n",
    "        # # print(ids.shape, target_verts.shape)\n",
    "        # # print(ids[0].shape)\n",
    "        \n",
    "        # target_ds = ds.reshape(-1,5)\n",
    "        # # print(target_verts[0],target_ds[0],tips_to_obj[0])\n",
    "        # # dis = torch.norm(target_verts - tips_to_obj,dim=-1)\n",
    "        # # print(dis[0],target_ds[0])\n",
    "        # # print(target_verts[0].shape,target_ds[0].shape)\n",
    "        # res = torch.cat((ids[0].unsqueeze(-1),target_ds[0].unsqueeze(-1)),dim=-1)\n",
    "        # # print(res.shape)\n",
    "        # # break\n",
    "        # # print(res)\n",
    "        # # print(res.shape)\n",
    "\n",
    "        # # break\n",
    "        # np.save(join(seq_path,'tips_closest_id_and_dis.npy'), res)\n",
    "        \"\"\"\n",
    "\n",
    "        # pc = tips_T[0]\n",
    "        # obj_rot = goal_obj_pose[:3,:3]\n",
    "        # obj_trans = goal_obj_pose[:3,3]\n",
    "        # pc = (obj_rot.T @ (pc - obj_trans).T).T\n",
    "        # nn = torch.from_numpy(kdt.query(pc.reshape(-1,3), k=1)[1])\n",
    "        # target_verts = obj_verts[nn.squeeze()]\n",
    "        # print(target_verts)\n",
    "        # print(pc)\n",
    "\n",
    "\n",
    "\n",
    "        # break\n",
    "       \n",
    "\n",
    "\n",
    "        # obj_T = obj_T.unsqueeze(1)\n",
    "        # tip_obj_dis = torch.norm(tips_T - obj_T,dim=-1).numpy()\n",
    "        # np.save(join(seq_path,'tip_distance.npy'), tip_obj_dis)\n",
    "\n",
    "        # hand_T = hand_params[goal_index:,:3]\n",
    "        # obj_T = obj_pose[goal_index:,:3,3]\n",
    "        # distance = torch.norm(hand_T-obj_T,dim=-1).numpy()\n",
    "        # np.save(join(seq_path,'global_distance.npy'), distance)\n",
    "        \n",
    "        # print(distanc\n",
    "        # seq_len = hand_params.shape[0] - goal_index\n",
    "        # lens.append(seq_len)\n",
    "\n",
    "\n",
    "        # print(tip_obj_dis.shape, tip_obj_dis[0])\n",
    "\n",
    "        # break\n",
    "        # joint_list.append(gt_joints)\n",
    "        # hand_motion = hand_params[:,:51]\n",
    "\n",
    "        # local_motion = global2local_axis_by_matrix(hand_motion.unsqueeze(0))\n",
    "        # local_motion_6d = axis2rot6d(local_motion)\n",
    "        # motion_list.append(local_motion_6d.squeeze(0))\n",
    "\n",
    "        # global_motion_6d = axis2rot6d(hand_motion.unsqueeze(0))\n",
    "        # hint_list.append(global_motion_6d.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345, 84)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(lens)\n",
    "lens.max(),lens.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hands_list[0].shape)\n",
    "local_motions = torch.cat(motion_list, dim=0).numpy()\n",
    "global_motions = torch.cat(hint_list, dim=0).numpy()\n",
    "# print(hand_motions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = global_motions.mean(axis=0)\n",
    "std = global_motions.std(axis=0)\n",
    "std[:3] = std[:3].mean() / 1.0\n",
    "std[3:6] = std[3:6].mean() / 1.0\n",
    "std[6:] = std[6:].mean() / 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/code/OmniControl/dataset/gazehoi_global_motion_6d_mean.npy', mean)\n",
    "np.save('/root/code/OmniControl/dataset/gazehoi_global_motion_6d_std.npy', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R0 = torch.eye(3)\n",
    "print(R0)\n",
    "matrix_to_axis_angle(R0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano',side='right',cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过物体轨迹反推手轨迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/765 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [00:47<00:00, 16.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import *\n",
    "from os.path import join\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "def get_trans_obj_verts(obj_verts,obj_pose):\n",
    "    \"\"\"\n",
    "    obj_verts: (N,3) -- T,N\n",
    "    obj_pose: (T,3,4)\n",
    "    \"\"\"\n",
    "    nf = obj_pose.shape[0]\n",
    "    N = obj_verts.shape[0]\n",
    "    # obj_pose = obj_pose.unsqueeze(1).repeat(1,N,1,1)\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    \n",
    "    obj_verts = obj_verts.unsqueeze(0).repeat(nf,1,1)\n",
    "    trans_obj_verts = torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T\n",
    "    return trans_obj_verts\n",
    "\n",
    "\n",
    "for seq in tqdm(valid_seqs):\n",
    "    seq_path = join('/root/code/seqs/1205_data/',seq)\n",
    "    meta_path =  join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    goal_index = meta['goal_index']\n",
    "    active_obj = meta['active_obj']\n",
    "    if meta['hand_flag'] == 'right':\n",
    "        hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy'))).float()\n",
    "        obj_pose = torch.tensor(np.load(join(seq_path,active_obj+'_pose_trans.npy'))).float()\n",
    "        obj_R = obj_pose[:,:3,:3]\n",
    "        # print(hand_params.shape, obj_pose.shape)\n",
    "        # obj_verts = torch.tensor(np.load(join(obj_path,active_obj,'resampled_500_trans.npy'))).float()\n",
    "        obj_verts = torch.tensor(trimesh.load(join(obj_path,active_obj,'simplified_scan_processed.obj')).vertices).float()\n",
    "        goal_obj_pose = obj_pose[goal_index]\n",
    "        # obj_verts = obj_verts @ goal_obj_pose[:3,:3].T + goal_obj_pose[:3,3].reshape(1,3)\n",
    "        # kdt = KDTree(obj_verts)\n",
    "        \n",
    "        hand_trans = hand_params[:,:3]\n",
    "        hand_rot = hand_params[:,3:6]\n",
    "        hand_theta = hand_params[:,3:51]\n",
    "        mano_beta = hand_params[:,51:]\n",
    "\n",
    "        goal_hand_trans = hand_trans[goal_index].unsqueeze(0)\n",
    "        trans_obj_verts = get_trans_obj_verts(obj_verts,obj_pose)\n",
    "        goal_obj_verts = trans_obj_verts[goal_index]\n",
    "        dis = torch.cdist(goal_hand_trans,goal_obj_verts)\n",
    "        min_dis,obj_idx = dis.min(dim=-1)\n",
    "\n",
    "        contact_normal = goal_hand_trans - goal_obj_verts[obj_idx]\n",
    "        reference_obj_rot = obj_R[goal_index]\n",
    "        update_hand_T = torch.zeros_like(hand_trans[goal_index:])\n",
    "        update_hand_R = torch.zeros_like(hand_rot[goal_index:])\n",
    "        obj_R = obj_R[goal_index:]\n",
    "        trans_obj_verts = trans_obj_verts[goal_index:]\n",
    "        for t_idx in range(len(hand_params)-goal_index):\n",
    "            relative_rot_mat = torch.matmul(obj_R[t_idx], reference_obj_rot.inverse())\n",
    "            curr_contact_normal = torch.matmul(relative_rot_mat, contact_normal.T).squeeze(-1)\n",
    "            update_hand_T[t_idx] = trans_obj_verts[t_idx, obj_idx] + curr_contact_normal\n",
    "        # print(update_hand_T.shape)\n",
    "        update_hand_T = update_hand_T.numpy()\n",
    "        np.save(join(seq_path,'contact_hand_T_from_obj_T.npy'), update_hand_T)\n",
    "        # print(hand_trans[goal_index:])\n",
    "        # print(hand_trans[goal_index:] - update_hand_T)\n",
    "        \n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OMOMO\n",
    "\n",
    "def process_hand_foot_contact_jpos(self, hand_foot_jpos, object_mesh_verts, obj_rot):\n",
    "    # hand_foot_jpos: T X 1 X 3 \n",
    "    # object_mesh_verts: T X Nv X 3 \n",
    "    # obj_rot: T X 3 X 3 \n",
    "    all_contact_labels = []\n",
    "    all_object_c_idx_list = []\n",
    "    all_dist = []\n",
    "\n",
    "    obj_rot = torch.from_numpy(obj_rot).to(hand_foot_jpos.device)\n",
    "    object_mesh_verts = object_mesh_verts.to(hand_foot_jpos.device)\n",
    "\n",
    "    num_joints = hand_foot_jpos.shape[1]\n",
    "    num_steps = hand_foot_jpos.shape[0]\n",
    "\n",
    "    threshold = 0.03 # Use palm position, should be smaller. \n",
    "    \n",
    "    joint2object_dist = torch.cdist(hand_foot_jpos, object_mesh_verts.to(hand_foot_jpos.device)) # T X 2 X Nv \n",
    "    \n",
    "    all_dist, all_object_c_idx_list = joint2object_dist.min(dim=2) # T X 2  最小距离，物体上最近点的id\n",
    "    all_contact_labels = all_dist < threshold # T X 2\n",
    "\n",
    "    new_hand_foot_jpos = hand_foot_jpos.clone() # T X 2 X 3 \n",
    "\n",
    "    # For each joint, scan the sequence, if contact is true, then use the corresponding object idx for the \n",
    "    # rest of subsequence in contact. \n",
    "    for j_idx in range(num_joints):\n",
    "        continue_prev_contact = False \n",
    "        for t_idx in range(num_steps):\n",
    "            if continue_prev_contact:\n",
    "                relative_rot_mat = torch.matmul(obj_rot[t_idx], reference_obj_rot.inverse())\n",
    "                curr_contact_normal = torch.matmul(relative_rot_mat, contact_normal[:, None]).squeeze(-1)\n",
    "\n",
    "                new_hand_foot_jpos[t_idx, j_idx] = object_mesh_verts[t_idx, subseq_contact_v_id] + \\\n",
    "                    curr_contact_normal  # 3  \n",
    "            \n",
    "            elif all_contact_labels[t_idx, j_idx] and not continue_prev_contact: # The first contact frame \n",
    "                subseq_contact_v_id = all_object_c_idx_list[t_idx, j_idx] # 物体上点的id\n",
    "                subseq_contact_pos = object_mesh_verts[t_idx, subseq_contact_v_id] # 3 开始接触这一帧，物体上和手在最近点的坐标\n",
    "\n",
    "                contact_normal = new_hand_foot_jpos[t_idx, j_idx] - subseq_contact_pos # Keep using this in the following frames. 手-物体\n",
    "\n",
    "                reference_obj_rot = obj_rot[t_idx] # 3 X 3 \n",
    "\n",
    "                continue_prev_contact = True \n",
    "\n",
    "    return new_hand_foot_jpos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't optimize a non-leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_473270/3453331561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 定义优化器，将切片参数添加到优化器中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msliced_params_to_optimize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 选择合适的优化算法和学习率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 在训练循环中，使用优化器来更新参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mdm/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mdm/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mdm/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    231\u001b[0m                                 \"but one of the params is \" + torch.typename(param))\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't optimize a non-leaf Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can't optimize a non-leaf Tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 假设您有一个包含一些参数的张量或变量\n",
    "# 这里假设参数是一个大小为(10, 10)的张量，我们将对其进行切片优化\n",
    "params = torch.randn(10, 10, requires_grad=True)\n",
    "\n",
    "# 定义要优化的切片\n",
    "sliced_params_to_optimize = params[:, :5]  # 例如，我们只优化第一列到第五列的切片\n",
    "\n",
    "# 定义优化器，将切片参数添加到优化器中\n",
    "optimizer = optim.SGD([sliced_params_to_optimize], lr=0.01)  # 选择合适的优化算法和学习率\n",
    "\n",
    "# 在训练循环中，使用优化器来更新参数\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # 清除之前的梯度\n",
    "    # 计算损失，这里假设您有一个损失函数\n",
    "    loss = compute_loss(sliced_params_to_optimize)\n",
    "    loss.backward()  # 计算梯度\n",
    "    optimizer.step()  # 更新参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查obj 表示方法转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175, 4, 3, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 3 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_473270/3615683418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mobj_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_pose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_pose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mobj_pose_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_global2local_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_pose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mobj_pose_local_6d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_matrix2rot6d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_pose_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mobj_pose_global_6d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_matrix2rot6d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_pose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_473270/3615683418.py\u001b[0m in \u001b[0;36mobj_global2local_matrix\u001b[0;34m(motion)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml_velocity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_velocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobj_global2local_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0ml_velocity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_velocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_get_vel_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# motion[:,1:,:3] = l_velocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# motion[:,1:,3:6] = r_velocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_473270/3615683418.py\u001b[0m in \u001b[0;36mobj_get_vel_matrix\u001b[0;34m(motion)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# motion = motion.reshape(-1,num_frames,3,4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrot_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmotion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0ml_velocity\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#bs, frames-1 ,3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 3 with size 3"
     ]
    }
   ],
   "source": [
    "from utils.data_util import  obj_matrix2rot6d\n",
    "from pytorch3d.transforms import  quaternion_to_axis_angle,axis_angle_to_quaternion, axis_angle_to_matrix, matrix_to_rotation_6d,rotation_6d_to_matrix,matrix_to_axis_angle\n",
    "def obj_get_vel_matrix(motion):\n",
    "    # bs, num_frames,3,4\n",
    "    # 相对最后一帧的表示\n",
    "    num_frames = motion.shape[1]\n",
    "    # motion = motion.reshape(-1,num_frames,3,4)\n",
    "    trans = motion[:,:,:3,3]\n",
    "    rot_matrix = motion[:,:,:3,:3]\n",
    "    l_velocity =  trans[:,:-1] - trans[:,1:] #bs, frames-1 ,3\n",
    "\n",
    "    matrix_1 = rot_matrix[:,:-1] # 需要求逆\n",
    "    matrix_0 = rot_matrix[:,1:]\n",
    "    matrix_0_inv = torch.einsum('...ij->...ji', [matrix_0])\n",
    "    r_velocity = torch.einsum('fipn,fink->fipk',matrix_1,matrix_0_inv)\n",
    "    # r_velocity = matrix_to_rotation_6d(r_velocity)\n",
    "    return l_velocity, r_velocity\n",
    "def obj_global2local_matrix(motion):\n",
    "    l_velocity, r_velocity = obj_get_vel_matrix(motion)\n",
    "    # motion[:,1:,:3] = l_velocity\n",
    "    # motion[:,1:,3:6] = r_velocity\n",
    "    new_motion = motion.clone()\n",
    "    new_motion[:, :-1, :3,3] = l_velocity\n",
    "    new_motion[:, :-1, :3,:3] = r_velocity\n",
    "    return new_motion\n",
    "def obj_local2global_rot6d_by_matrix(motion):\n",
    "     # 只对global RT做相对变换\n",
    "    #  反过来 最后一帧是参考\n",
    "    l_velocity = torch.flip(motion[:,:,:3],dims=[1])\n",
    "    r_velocity = motion[:,:,3:9]\n",
    "\n",
    "    trans = torch.flip(torch.cumsum(l_velocity,dim=1),dims=[1])\n",
    "\n",
    "    r_velocity = rotation_6d_to_matrix(r_velocity)\n",
    "    print(r_velocity.shape)\n",
    "    nf = l_velocity.shape[1]\n",
    "    rot = r_velocity[:,-1].unsqueeze(1) #bs,1,3,3\n",
    "    cur_r = rot\n",
    "    for i in range(nf-2,-1,-1):\n",
    "        # print(i)\n",
    "        rot_i = torch.einsum('fipn,fink->fipk',r_velocity[:,i].unsqueeze(1),cur_r)\n",
    "        cur_r = rot_i\n",
    "        rot = torch.cat((rot_i,rot),dim=1)\n",
    "\n",
    "    rot = matrix_to_rotation_6d(rot)\n",
    "    new_motion = motion.clone()\n",
    "    new_motion[:,:,:3] = trans\n",
    "    new_motion[:,:,3:9] = rot\n",
    "    return new_motion\n",
    "\n",
    "def obj_local2global_matrix(motion):\n",
    "     # 只对global RT做相对变换\n",
    "    #  反过来 最后一帧是参考\n",
    "    \n",
    "    bs,nf ,_= motion.shape\n",
    "    new_motion = torch.zeros((bs,nf,4,3,4))\n",
    "    motion = motion.reshape(bs,nf,4,9)\n",
    "    new_motion = motion.clone()\n",
    "    for k in range(4):\n",
    "        l_velocity = torch.flip(motion[:,:,k,:3],dims=[1])\n",
    "        r_velocity = motion[:,:,k,3:9]\n",
    "\n",
    "        trans = torch.flip(torch.cumsum(l_velocity,dim=1),dims=[1])\n",
    "\n",
    "        r_velocity = rotation_6d_to_matrix(r_velocity)\n",
    "        nf = l_velocity.shape[1]\n",
    "        rot = r_velocity[:,-1].unsqueeze(1) #bs,1,3,3\n",
    "        cur_r = rot\n",
    "        for i in range(nf-2,-1,-1):\n",
    "            # print(i)\n",
    "            rot_i = torch.einsum('fipn,fink->fipk',r_velocity[:,i].unsqueeze(1),cur_r)\n",
    "            cur_r = rot_i\n",
    "            rot = torch.cat((rot_i,rot),dim=1)\n",
    "\n",
    "        # rot = matrix_to_rotation_6d(rot)\n",
    "        \n",
    "        new_motion[:,:,k,:3,3] = trans\n",
    "        new_motion[:,:,k,:3,:3] = rot\n",
    "    # new_motion = new_motion.reshape(bs,nf,-1)\n",
    "    return new_motion\n",
    "obj_pose = np.load('/root/code/seqs/1205_data/0001/001_book_4_pose.npy')\n",
    "bs = obj_pose.shape[0]\n",
    "nf = obj_pose.shape[1]\n",
    "obj_pose = torch.tensor(obj_pose).unsqueeze(1).repeat(1,4,1,1).reshape()\n",
    "print(obj_pose.shape)\n",
    "obj_pose_local = obj_global2local_matrix(obj_pose.unsqueeze(0))\n",
    "obj_pose_local_6d = obj_matrix2rot6d(obj_pose_local)\n",
    "obj_pose_global_6d = obj_matrix2rot6d(obj_pose.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "new_global_matrix = obj_local2global_matrix(obj_pose_local_6d).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 9])\n",
      "torch.Size([150, 3, 3]) torch.Size([150, 3, 3])\n",
      "tensor([[[ 1.0000e+00,  7.7716e-15,  2.5369e-14],\n",
      "         [-7.7993e-15,  1.0000e+00, -4.6629e-14],\n",
      "         [-2.5313e-14,  4.6602e-14,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  7.4662e-15,  2.4869e-14],\n",
      "         [-7.4940e-15,  1.0000e+00, -4.6324e-14],\n",
      "         [-2.4925e-14,  4.6296e-14,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  7.6605e-15,  2.5036e-14],\n",
      "         [-7.6883e-15,  1.0000e+00, -4.5908e-14],\n",
      "         [-2.5091e-14,  4.5935e-14,  1.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000e+00,  2.7756e-17,  4.4409e-16],\n",
      "         [-8.3267e-17,  1.0000e+00, -5.2736e-16],\n",
      "         [-3.8858e-16,  5.5511e-16,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.1102e-16,  2.7756e-16],\n",
      "         [-8.3267e-17,  1.0000e+00, -5.5511e-17],\n",
      "         [-2.2204e-16,  8.3267e-17,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  8.3267e-17, -5.5511e-17],\n",
      "         [-5.5511e-17,  1.0000e+00,  5.5511e-17],\n",
      "         [ 5.5511e-17,  0.0000e+00,  1.0000e+00]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(new_global_6d.shape)\n",
    "old = rotation_6d_to_matrix(obj_pose_global_6d[:,3:])\n",
    "new = rotation_6d_to_matrix(new_global_6d[:,3:])\n",
    "print(old.shape, new.shape)\n",
    "matrix_0_inv = torch.einsum('...ij->...ji', [old])\n",
    "r_velocity = torch.einsum('ipn,ink->ipk',new,matrix_0_inv)\n",
    "print(r_velocity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
