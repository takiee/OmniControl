{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/render/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyrender\n",
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "import pickle\n",
    "from pysdf import SDF\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from tqdm import *\n",
    "import cv2\n",
    "import torch\n",
    "from manotorch.manolayer import ManoLayer\n",
    "os.environ['MUJOCO_GL'] = 'osmesa'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "def vis_smpl(out_path, image, nf, vertices, faces, camera_R, camera_T, K = np.array([1031.8450927734375, 0.0, 932.1596069335938, 0.0, 1022.4588623046875, 541.9437255859375, 0.0, 0.0, 1.0]).reshape((3,3))):\n",
    "    # outname = os.path.join(out_path, '{:06d}.jpg'.format(nf))\n",
    "    # if os.path.exists(outname): return\n",
    "    render_data = {}\n",
    "    assert vertices.shape[1] == 3 and len(vertices.shape) == 2, 'shape {} != (N, 3)'.format(vertices.shape)\n",
    "    render_data = {\"vertices\": vertices, \"faces\": faces, \"vid\":0, \"name\": \"human_{}_0\".format(nf)}\n",
    "\n",
    "    camera = {\"K\": K,\n",
    "        \"R\": camera_R,\n",
    "        \"T\":camera_T}\n",
    "    from visualize.renderer_raw import Renderer\n",
    "    render = Renderer(height=3840, width=2160, faces=None)\n",
    "    image_vis, depth = render.render(render_data, camera, image, add_back=True)\n",
    "    # print(depth)\n",
    "    if nf=='gt':\n",
    "        outname = os.path.join(out_path, nf+'.jpg')\n",
    "    else: \n",
    "        outname = os.path.join(out_path, nf.zfill(4)+'.jpg')\n",
    "    cv2.imwrite(outname, image_vis)\n",
    "    return image_vis, depth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contactgen_path = 'final_results/contactgen_500_2000'\n",
    "seqs = os.listdir(contactgen_path)\n",
    "for seq in seqs:\n",
    "    seq_path = join(contactgen_path,seq)\n",
    "    res_path = join(seq_path, 'results')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opt 不优化R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_path = \"/root/code/seqs/calibration_all.json\"\n",
    "with open(calib_path) as f:\n",
    "    calib_dome = json.load(f)\n",
    "    f.close()\n",
    "view = '1'\n",
    "camera_pose = np.vstack((np.asarray(calib_dome[str(1)]['RT']).reshape((3,4)), np.ones(4) ))\n",
    "K = np.asarray(calib_dome[str(1)]['K']).reshape((3,3))\n",
    "\n",
    "manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano',side='right')\n",
    "hand_faces = manolayer.th_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hand_motion', 'obj_motion', 'seq'])\n",
      "0071\n",
      "optim/val_gaze/0071render_all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3356364/3909446944.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obj_pose = torch.tensor(res['obj_motion']).cpu().float()\n"
     ]
    }
   ],
   "source": [
    "datapath = '/root/code/seqs/0303_data/'\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "path = 'optim/val_gaze/0071.npy'\n",
    "# path = 'optim/val_gaze/0071_no_opt_R.npy'\n",
    "res = np.load((path),allow_pickle=True).item()\n",
    "print(res.keys())\n",
    "pred_motion = torch.tensor(res['hand_motion']).cpu()\n",
    "obj_pose = torch.tensor(res['obj_motion']).cpu().float()\n",
    "seq = res['seq']\n",
    "print(seq)\n",
    "seq_path = join(datapath,seq)\n",
    "\n",
    "meta_path = join(seq_path,'meta.pkl')\n",
    "with open(meta_path,'rb')as f:\n",
    "    meta = pickle.load(f)\n",
    "    \n",
    "active_obj = meta['active_obj']\n",
    "obj_mesh_path = join(obj_path,active_obj,'simplified_scan_processed.obj')\n",
    "obj_mesh = trimesh.load(obj_mesh_path)\n",
    "obj_verts = obj_mesh.vertices\n",
    "obj_faces = obj_mesh.faces\n",
    "\n",
    "seq_len = obj_pose.shape[0]\n",
    "obj_verts = torch.tensor(obj_verts).unsqueeze(0).repeat(seq_len,1,1).float()\n",
    "\n",
    "\n",
    "pred_trans = pred_motion[:,:3]\n",
    "pred_theta = pred_motion[:,3:51]\n",
    "pred_rot = pred_motion[:,3:6]\n",
    "length = pred_motion.shape[0]\n",
    "\n",
    "\n",
    "hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "mano_beta = hand_params[0,51:].unsqueeze(0).repeat(seq_len,1)\n",
    "\n",
    "pred_output = manolayer(pred_theta, mano_beta)\n",
    "pred_verts = pred_output.verts  + pred_trans.unsqueeze(1)\n",
    "# pred_verts = pred_output.verts - pred_output.joints[:, 0].unsqueeze(1) + pred_trans.unsqueeze(1)\n",
    "\n",
    "\n",
    "obj_R = obj_pose[:,:3,:3]\n",
    "obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "\n",
    "obj_verts = torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T\n",
    "\n",
    "# print(obj_verts.shape,mano_verts.shape)\n",
    "render_path = path.replace('.npy','render_all')\n",
    "print(render_path)\n",
    "render_out = join(render_path,seq,str(view))\n",
    "os.makedirs(render_out,exist_ok=True)\n",
    "for k in range(len(pred_verts)):\n",
    "    img = np.zeros((2160, 3840,3), np.uint8) + 255\n",
    "    # print(k)\n",
    "    pred_all_verts = np.vstack((pred_verts[k].numpy(),obj_verts[k].numpy()))\n",
    "    faces = np.vstack((hand_faces,obj_faces+778))\n",
    "    pred_img, _ = vis_smpl(render_out, img, str(k), pred_all_verts, faces,camera_pose[:3,:3],camera_pose[:3,3],K)\n",
    "    # outname = os.path.join(render_out, '{:06d}.jpg'.format(k))\n",
    "    # cv2.imwrite(outname, pred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/root/code/seqs/gazehoi_list_test_0303.txt'\n",
    "with open(datapath,'r') as f:\n",
    "    info_list = f.readlines()\n",
    "test_seqs = []\n",
    "for info in info_list:\n",
    "    seq = info.strip()\n",
    "    test_seqs.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_seqs 批量\n",
    "\n",
    "datapath = '/root/code/seqs/0303_data/'\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "for seq in tqdm(test_seqs):\n",
    "    path = f'optim/val_gaze/{seq}.npy'\n",
    "    # path = 'optim/val_gaze/0071_no_opt_R.npy'\n",
    "    res = np.load((path),allow_pickle=True).item()\n",
    "    print(res.keys())\n",
    "    pred_motion = torch.tensor(res['hand_motion']).cpu()\n",
    "    obj_pose = torch.tensor(res['obj_motion']).cpu().float()\n",
    "    seq = res['seq']\n",
    "    print(seq)\n",
    "    seq_path = join(datapath,seq)\n",
    "\n",
    "    meta_path = join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "        \n",
    "    active_obj = meta['active_obj']\n",
    "    obj_mesh_path = join(obj_path,active_obj,'simplified_scan_processed.obj')\n",
    "    obj_mesh = trimesh.load(obj_mesh_path)\n",
    "    obj_verts = obj_mesh.vertices\n",
    "    obj_faces = obj_mesh.faces\n",
    "\n",
    "    seq_len = obj_pose.shape[0]\n",
    "    obj_verts = torch.tensor(obj_verts).unsqueeze(0).repeat(seq_len,1,1).float()\n",
    "\n",
    "\n",
    "    pred_trans = pred_motion[:,:3]\n",
    "    pred_theta = pred_motion[:,3:51]\n",
    "    pred_rot = pred_motion[:,3:6]\n",
    "    length = pred_motion.shape[0]\n",
    "\n",
    "\n",
    "    hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "    mano_beta = hand_params[0,51:].unsqueeze(0).repeat(seq_len,1)\n",
    "\n",
    "    pred_output = manolayer(pred_theta, mano_beta)\n",
    "    pred_verts = pred_output.verts  + pred_trans.unsqueeze(1)\n",
    "    # pred_verts = pred_output.verts - pred_output.joints[:, 0].unsqueeze(1) + pred_trans.unsqueeze(1)\n",
    "\n",
    "\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "\n",
    "    obj_verts = torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T\n",
    "\n",
    "    # print(obj_verts.shape,mano_verts.shape)\n",
    "    render_path = path.replace('.npy','render_all')\n",
    "    print(render_path)\n",
    "    render_out = join(render_path,seq,str(view))\n",
    "    os.makedirs(render_out,exist_ok=True)\n",
    "    for k in range(len(pred_verts)):\n",
    "        img = np.zeros((2160, 3840,3), np.uint8) + 255\n",
    "        # print(k)\n",
    "        pred_all_verts = np.vstack((pred_verts[k].numpy(),obj_verts[k].numpy()))\n",
    "        faces = np.vstack((hand_faces,obj_faces+778))\n",
    "        pred_img, _ = vis_smpl(render_out, img, str(k), pred_all_verts, faces,camera_pose[:3,:3],camera_pose[:3,3],K)\n",
    "        # outname = os.path.join(render_out, '{:06d}.jpg'.format(k))\n",
    "        # cv2.imwrite(outname, pred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_ok_seq = ['0009',\n",
    "                '0011',\n",
    "                '0038',\n",
    "                '0071',\n",
    "                '0198',\n",
    "                '0260',\n",
    "                '0390',\n",
    "                '0412',\n",
    "                '0413',\n",
    "                '0421',\n",
    "                '0425',\n",
    "                '0436',\n",
    "                '0522',\n",
    "                '0535',\n",
    "                '0536',\n",
    "                '0569',\n",
    "                '0644',\n",
    "                '0670',\n",
    "                '0681',\n",
    "                '0746',\n",
    "                '0773',\n",
    "                '0796',\n",
    "                '0808',\n",
    "                '0827',\n",
    "                '0957',\n",
    "                '1004',\n",
    "                '1009',\n",
    "                '1020',\n",
    "                '1059',\n",
    "                '1246',\n",
    "                '1282',\n",
    "                '1294',\n",
    "                '1299',\n",
    "                '1306',\n",
    "                '1313',\n",
    "                '1315',\n",
    "                '1317',\n",
    "                '1352'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stage1_ok_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_ok_seq = ['0009',\n",
    "                '0038',\n",
    "                '0071', # view3 ok\n",
    "                '0260',\n",
    "                '0436',\n",
    "                '0522',\n",
    "                '0681',\n",
    "                '0746',\n",
    "                '1059',\n",
    "                '1246', # s2前半部分ok\n",
    "                '1306',\n",
    "                '1315' # s2前半部分ok\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_ok_seq = ['0009', # view8 ok\n",
    "                '0038',\n",
    "                '0071', # view8 ok\n",
    "                '0260', # view8 ok\n",
    "                '0436', # view8 ok\n",
    "                '0522',\n",
    "                '0681', # view8 ok\n",
    "                '0746',\n",
    "                '1246', # s2前半部分ok\n",
    "                '1306',\n",
    "                '1315' # s2前半部分ok # view8 ok\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new 0312_val_gaze_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_ok = ['0009',\n",
    "             '0014',\n",
    "             '',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_mianqiang_ok = ['0071']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_ok_opt_chuanmo = ['0013']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用手物不同颜色把ours结果可视化出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个物体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_311642/2401349127.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obj_pose = torch.tensor(res['obj_motion']).cpu().float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1538, 3])\n",
      "['003_vase_1', '003_vase_2', '003_vase_3', '003_vase_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [02:15<00:00,  1.20it/s]\n",
      "100%|██████████| 163/163 [00:19<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['006_apple', '006_pear', '006_lemon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [02:14<00:00,  1.19it/s]\n",
      "100%|██████████| 160/160 [00:18<00:00,  8.80it/s]\n"
     ]
    }
   ],
   "source": [
    "def vis_smpl_color(out_path, image, nf, vertices, faces, camera_R, camera_T, K,index=778,face_index=1538):\n",
    "    outname = os.path.join(out_path, '{:06d}.jpg'.format(nf))\n",
    "    # if os.path.exists(outname): return\n",
    "    render_data = {}\n",
    "    assert vertices.shape[1] == 3 and len(vertices.shape) == 2, 'shape {} != (N, 3)'.format(vertices.shape)\n",
    "    # render_data = {\"vertices\": vertices, \"faces\": faces, \"vid\":0, \"name\": \"human_{}_0\".format(nf)}\n",
    "    render_data_hand = {\"vertices\": vertices[:index], \"faces\": faces[:face_index], \"vid\":0, \"name\": \"human_{}_0_h\".format(nf),\"col_code\":(60,100,160)}\n",
    "    render_data_obj = {\"vertices\": vertices[index:], \"faces\": faces[face_index:]-778, \"vid\":0, \"name\": \"human_{}_0_o\".format(nf),\"col_code\":(180, 140, 40)}\n",
    "    #(112,128,144)\n",
    "    camera = {\"K\": K,\n",
    "        \"R\": camera_R,\n",
    "        \"T\":camera_T}\n",
    "    from visualize.renderer import Renderer\n",
    "    render = Renderer(height=3840, width=2160, faces=None)\n",
    "    image_vis, depth = render.render([render_data_hand,render_data_obj], camera, image, add_back=True)\n",
    "    # print(depth)\n",
    "    # outname = os.path.join(out_path, '{:06d}.jpg'.format(nf))\n",
    "    # cv2.imwrite(outname, image_vis)\n",
    "    return image_vis, depth  \n",
    "\n",
    "def img2video(image_path, video_path):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 用于mp4格式的生成\n",
    "    height = 2160\n",
    "    weight = 3840\n",
    "    fps = 10\n",
    "    videowriter = cv2.VideoWriter(video_path, fourcc, fps, (weight, height))  # 创建一个写入视频对象\n",
    "\n",
    "    imgs = sorted(os.listdir(image_path))\n",
    "    for img in tqdm(imgs):\n",
    "        frame = cv2.imread(join(image_path, img))\n",
    "        videowriter.write(frame)\n",
    "    videowriter.release()\n",
    "\n",
    "\n",
    "datapath = '/root/code/seqs/0303_data/'\n",
    "res_path = 'final_results/check_vis'\n",
    "manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano',side='right')\n",
    "hand_faces = manolayer.th_faces\n",
    "print(hand_faces.shape)\n",
    "\n",
    "calib_path = \"/root/code/seqs/calibration_all.json\"\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "with open(calib_path) as f:\n",
    "    calib_dome = json.load(f)\n",
    "    f.close()\n",
    "view = 1\n",
    "camera_pose = np.vstack((np.asarray(calib_dome[str(view)]['RT']).reshape((3,4)), np.ones(4) ))\n",
    "K = np.asarray(calib_dome[str(view)]['K']).reshape((3,3))\n",
    "\n",
    "# seqs = ['0009','0071','0155','0327','0553','0641','0664','1016','1327','1354']\n",
    "seqs = ['1016','0641']\n",
    "for seq in seqs:\n",
    "    # break\n",
    "    pred_path = join(res_path,seq,f'{seq}.npy')\n",
    "    res = np.load((pred_path),allow_pickle=True).item()\n",
    "    pred_motion = torch.tensor(res['hand_motion']).cpu()\n",
    "    obj_pose = torch.tensor(res['obj_motion']).cpu().float()\n",
    "\n",
    "    seq_path = join(datapath,seq)\n",
    "\n",
    "    meta_path = join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "            meta = pickle.load(f)\n",
    "        \n",
    "    active_obj = meta['active_obj']\n",
    "    obj_name_list = meta['obj_name_list']\n",
    "    print(obj_name_list)\n",
    "    obj_mesh_path = join(obj_path,active_obj,'simplified_scan_processed.obj')\n",
    "    obj_mesh = trimesh.load(obj_mesh_path)\n",
    "    obj_verts = obj_mesh.vertices\n",
    "    obj_faces = obj_mesh.faces\n",
    "    \n",
    "    seq_len = obj_pose.shape[0]\n",
    "    obj_verts = torch.tensor(obj_verts).unsqueeze(0).repeat(seq_len,1,1).float()\n",
    "    \n",
    "    \n",
    "    pred_trans = pred_motion[:,:3]\n",
    "    pred_theta = pred_motion[:,3:51]\n",
    "    pred_rot = pred_motion[:,3:6]\n",
    "    length = pred_motion.shape[0]\n",
    "\n",
    "    hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "    mano_beta = hand_params[0,51:].unsqueeze(0).repeat(seq_len,1)\n",
    "\n",
    "    pred_output = manolayer(pred_theta, mano_beta)\n",
    "    pred_verts = pred_output.verts - pred_output.joints[:, 0].unsqueeze(1) + pred_trans.unsqueeze(1)\n",
    "\n",
    "\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    \n",
    "    obj_verts = torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T\n",
    "\n",
    "    render_path = pred_path.replace('.npy','render_color')\n",
    "    render_out = join(render_path,seq,str(view))\n",
    "    os.makedirs(render_out,exist_ok=True)\n",
    "    for k in tqdm(range(len(pred_verts))):\n",
    "        img = np.zeros((2160, 3840,3), np.uint8) + 255\n",
    "        # print(k)\n",
    "        pred_all_verts = np.vstack((pred_verts[k].numpy(),obj_verts[k].numpy()))\n",
    "        faces = np.vstack((hand_faces,obj_faces+778))\n",
    "        pred_img, _ = vis_smpl_color(render_out, img, k, pred_all_verts, faces,camera_pose[:3,:3],camera_pose[:3,3],K)\n",
    "        outname = os.path.join(render_out, '{:06d}.jpg'.format(k))\n",
    "        # print(outname)\n",
    "        cv2.imwrite(outname, pred_img)\n",
    "        # break\n",
    "    # break\n",
    "    video_name = 'seq' + seq + '_view' + str(view) + '.mp4' \n",
    "    video_path = join(render_path,video_name)\n",
    "    img2video(render_out,video_path)\n",
    "    # video_name = 'seq' + seq + '_view' + str(view) + '.mp4' \n",
    "    # video_path = join(render_path,'render_videos')\n",
    "    # os.makedirs(video_path,exist_ok=True)\n",
    "    # video_path = join(video_path,video_name)\n",
    "    # img2video(render_out,video_path)\n",
    "    # # seq_index += 1\n",
    "    # # if render_num == seq_index:\n",
    "    # #     break\n",
    "    # # break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多个物体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1538, 3])\n",
      "['006_banana', '006_apple', '002_plastic_furniture_top', '002_plastic_furniture_bottom']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_311642/3040301727.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obj_pose = torch.tensor(res['obj_motion']).cpu().float()[:seq_len]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 43726, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [03:25<00:00,  1.05s/it]\n",
      "100%|██████████| 284/284 [00:32<00:00,  8.70it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datapath = '/root/code/seqs/0303_data/'\n",
    "res_path = 'final_results/check_vis'\n",
    "manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano',side='right')\n",
    "hand_faces = manolayer.th_faces\n",
    "print(hand_faces.shape)\n",
    "\n",
    "calib_path = \"/root/code/seqs/calibration_all.json\"\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "with open(calib_path) as f:\n",
    "    calib_dome = json.load(f)\n",
    "    f.close()\n",
    "view = 1\n",
    "camera_pose = np.vstack((np.asarray(calib_dome[str(view)]['RT']).reshape((3,4)), np.ones(4) ))\n",
    "K = np.asarray(calib_dome[str(view)]['K']).reshape((3,3))\n",
    "\n",
    "seqs = ['1327']\n",
    "# seqs = ['0009','0071','0155','0327','0553','0641','0664','1016','1327','1354']\n",
    "for seq in seqs:\n",
    "    # break\n",
    "    seq_len = 195\n",
    "    pred_path = join(res_path,seq,f'{seq}.npy')\n",
    "    res = np.load((pred_path),allow_pickle=True).item()\n",
    "    pred_motion = torch.tensor(res['hand_motion']).cpu()[:seq_len]\n",
    "    obj_pose = torch.tensor(res['obj_motion']).cpu().float()[:seq_len]\n",
    "\n",
    "    seq_path = join(datapath,seq)\n",
    "\n",
    "    meta_path = join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "            meta = pickle.load(f)\n",
    "        \n",
    "    active_obj = meta['active_obj']\n",
    "    obj_name_list = meta['obj_name_list']\n",
    "    print(obj_name_list)\n",
    "\n",
    "    obj_mesh_path = join(obj_path,active_obj,'simplified_scan_processed.obj')\n",
    "    obj_mesh = trimesh.load(obj_mesh_path)\n",
    "    obj_verts = obj_mesh.vertices\n",
    "    obj_faces = obj_mesh.faces\n",
    "    \n",
    "    # seq_len = obj_pose.shape[0]\n",
    "    obj_verts = torch.tensor(obj_verts).unsqueeze(0).repeat(seq_len,1,1).float()\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    obj_verts = (torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T).numpy()\n",
    "\n",
    "    for obj in obj_name_list:\n",
    "        if obj == active_obj:\n",
    "            continue\n",
    "        obj_mesh_path = join(obj_path,obj,'simplified_scan_processed.obj')\n",
    "        obj_mesh = trimesh.load(obj_mesh_path)\n",
    "        obj_verts_ = obj_mesh.vertices\n",
    "        obj_faces_ = obj_mesh.faces\n",
    "        \n",
    "        obj_verts_ = torch.tensor(obj_verts_).unsqueeze(0).repeat(seq_len,1,1).float()\n",
    "        obj_pose_ = torch.tensor(np.load(join(seq_path,obj+'_pose_trans.npy'))).reshape(-1,3,4)[:seq_len].float()\n",
    "        obj_R = obj_pose_[:,:3,:3]\n",
    "        obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "        obj_T = obj_pose_[:,:3,3].unsqueeze(1)\n",
    "        obj_verts_ = (torch.einsum('fpn,fnk->fpk',obj_verts_,obj_R) + obj_T).numpy()\n",
    "        obj_faces = np.vstack((obj_faces,obj_faces_+obj_verts.shape[1]))\n",
    "        obj_verts = np.concatenate((obj_verts,obj_verts_),axis=1)\n",
    "    print(obj_verts.shape)\n",
    "    \n",
    "    pred_trans = pred_motion[:,:3]\n",
    "    pred_theta = pred_motion[:,3:51]\n",
    "    pred_rot = pred_motion[:,3:6]\n",
    "    length = pred_motion.shape[0]\n",
    "\n",
    "    hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "    mano_beta = hand_params[0,51:].unsqueeze(0).repeat(seq_len,1)\n",
    "\n",
    "    pred_output = manolayer(pred_theta, mano_beta)\n",
    "    pred_verts = pred_output.verts - pred_output.joints[:, 0].unsqueeze(1) + pred_trans.unsqueeze(1)\n",
    "\n",
    "\n",
    "    render_path = pred_path.replace('.npy','render_color_multi_obj')\n",
    "    render_out = join(render_path,seq,str(view))\n",
    "    os.makedirs(render_out,exist_ok=True)\n",
    "    for k in tqdm(range(len(pred_verts))):\n",
    "        img = np.zeros((2160, 3840,3), np.uint8) + 255\n",
    "        # print(k)\n",
    "        pred_all_verts = np.vstack((pred_verts[k].numpy(),obj_verts[k]))\n",
    "        faces = np.vstack((hand_faces,obj_faces+778))\n",
    "        pred_img, _ = vis_smpl_color(render_out, img, k, pred_all_verts, faces,camera_pose[:3,:3],camera_pose[:3,3],K)\n",
    "        outname = os.path.join(render_out, '{:06d}.jpg'.format(k))\n",
    "        # print(outname)\n",
    "        cv2.imwrite(outname, pred_img)\n",
    "        # break\n",
    "    video_name = 'seq' + seq + '_view' + str(view) + '.mp4' \n",
    "    video_path = join(render_path,video_name)\n",
    "    img2video(render_out,video_path)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 345/345 [00:38<00:00,  8.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# seqs = ['0155','0327','1016','0641']\n",
    "seqs = ['0009']\n",
    "for seq in seqs:\n",
    "    print(seq)\n",
    "    render_path = f'final_results/check_vis/{seq}/{seq}render_color_multi_obj/{seq}/1/'\n",
    "    video_name = 'seq_10fps' + seq + '_view' + str(view) + '.mp4' \n",
    "    video_path = join(f'final_results/check_vis/{seq}/',video_name)\n",
    "    img2video(render_path,video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 渲染g2ho和o2h的多物体带颜色结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1538, 3])\n",
      "torch.Size([195, 51]) torch.Size([195, 3, 4])\n",
      "['006_banana', '006_apple', '002_plastic_furniture_top', '002_plastic_furniture_bottom']\n",
      "torch.Size([195, 10002, 3]) torch.Size([195, 3, 4])\n",
      "torch.Size([195, 10004, 3]) torch.Size([195, 3, 4])\n",
      "torch.Size([195, 10008, 3]) torch.Size([195, 3, 4])\n",
      "(195, 43726, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [03:15<00:00,  1.00s/it]\n",
      "100%|██████████| 196/196 [00:22<00:00,  8.77it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datapath = '/root/code/seqs/0303_data/'\n",
    "# res_path = 'final_results/check_vis/g2ho'\n",
    "res_path = 'final_results/check_vis/o2h'\n",
    "manolayer = ManoLayer(mano_assets_root='/root/code/CAMS/data/mano_assets/mano',side='right')\n",
    "hand_faces = manolayer.th_faces\n",
    "print(hand_faces.shape)\n",
    "\n",
    "calib_path = \"/root/code/seqs/calibration_all.json\"\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "with open(calib_path) as f:\n",
    "    calib_dome = json.load(f)\n",
    "    f.close()\n",
    "view = 1\n",
    "camera_pose = np.vstack((np.asarray(calib_dome[str(view)]['RT']).reshape((3,4)), np.ones(4) ))\n",
    "K = np.asarray(calib_dome[str(view)]['K']).reshape((3,3))\n",
    "\n",
    "select_seqs = ['1327']\n",
    "# path = 'save/0304_g2ho/samples_0304_g2ho_000150000_seed10_0307_test/results.npy'\n",
    "path = 'save/0303_o2h/samples_0303_o2h_000150000_seed10_0307_test/results.npy'\n",
    "# seqs = ['0009','0071','0155','0327','0553','0641','0664','1016','1327','1354']\n",
    "res = np.load(path,allow_pickle=True).item()\n",
    "all_hand_motions = torch.tensor(res['all_hand_motions']).cpu().float() # obj_motion (bs,nf,4,3,4)\n",
    "all_obj_motions = torch.tensor(res['all_obj_motions']).cpu().float() # obj_motion (bs,nf,4,3,4)\n",
    "seqs = res['seqs']\n",
    "for seq in select_seqs:\n",
    "    # break\n",
    "    i = seqs.index(seq)\n",
    "    # print(i)\n",
    "    # print(seqs[i])\n",
    "    # continue\n",
    "    # pred_path = join(res_path,seq,f'{seq}.npy')\n",
    "    # res = np.load((pred_path),allow_pickle=True).item()\n",
    "    if seq == '0009':\n",
    "        seq_len = 345\n",
    "    if seq == '0155':\n",
    "        seq_len = 150\n",
    "    if seq == '0327':\n",
    "        seq_len = 155\n",
    "    if seq == '0641':\n",
    "        seq_len = 90\n",
    "    if seq == '1016':\n",
    "        seq_len = 163\n",
    "    if seq == '1313':\n",
    "        seq_len = 80\n",
    "    if seq == '1327':\n",
    "        seq_len = 195\n",
    "    pred_motion = all_hand_motions[i][:seq_len]\n",
    "    obj_pose = all_obj_motions[i][:seq_len]\n",
    "    print(pred_motion.shape,obj_pose.shape)\n",
    "\n",
    "    seq_path = join(datapath,seq)\n",
    "\n",
    "    meta_path = join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "            meta = pickle.load(f)\n",
    "        \n",
    "    active_obj = meta['active_obj']\n",
    "    obj_name_list = meta['obj_name_list']\n",
    "    print(obj_name_list)\n",
    "\n",
    "    obj_mesh_path = join(obj_path,active_obj,'simplified_scan_processed.obj')\n",
    "    obj_mesh = trimesh.load(obj_mesh_path)\n",
    "    obj_verts = obj_mesh.vertices\n",
    "    obj_faces = obj_mesh.faces\n",
    "    \n",
    "    \n",
    "    # seq_len = obj_pose.shape[0]\n",
    "    obj_verts = torch.tensor(obj_verts).unsqueeze(0).repeat(seq_len,1,1).float()\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    obj_verts = (torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T).numpy()\n",
    "\n",
    "    for obj in obj_name_list:\n",
    "        if obj == active_obj:\n",
    "            continue\n",
    "        obj_mesh_path = join(obj_path,obj,'simplified_scan_processed.obj')\n",
    "        obj_mesh = trimesh.load(obj_mesh_path)\n",
    "        obj_verts_ = obj_mesh.vertices\n",
    "        obj_faces_ = obj_mesh.faces\n",
    "        \n",
    "        obj_verts_ = torch.tensor(obj_verts_).unsqueeze(0).repeat(seq_len,1,1).float()\n",
    "        obj_pose_ = torch.tensor(np.load(join(seq_path,obj+'_pose_trans.npy'))).reshape(-1,3,4)[0].repeat(seq_len,1,1).float()\n",
    "        obj_R = obj_pose_[:,:3,:3]\n",
    "        obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "        obj_T = obj_pose_[:,:3,3].unsqueeze(1)\n",
    "        print(obj_verts_.shape,obj_pose_.shape)\n",
    "        obj_verts_ = (torch.einsum('fpn,fnk->fpk',obj_verts_,obj_R) + obj_T).numpy()\n",
    "        obj_faces = np.vstack((obj_faces,obj_faces_+obj_verts.shape[1]))\n",
    "        obj_verts = np.concatenate((obj_verts,obj_verts_),axis=1)\n",
    "    print(obj_verts.shape)\n",
    "    \n",
    "    pred_trans = pred_motion[:,:3]\n",
    "    pred_theta = pred_motion[:,3:51]\n",
    "    pred_rot = pred_motion[:,3:6]\n",
    "    length = pred_motion.shape[0]\n",
    "\n",
    "    hand_params = torch.tensor(np.load(join(seq_path,'mano/poses_right.npy')))\n",
    "    mano_beta = hand_params[0,51:].unsqueeze(0).repeat(seq_len,1)\n",
    "\n",
    "    pred_output = manolayer(pred_theta, mano_beta)\n",
    "    pred_verts = pred_output.verts - pred_output.joints[:, 0].unsqueeze(1) + pred_trans.unsqueeze(1)\n",
    "\n",
    "\n",
    "    render_out = join(res_path,seq,str(view))\n",
    "    os.makedirs(render_out,exist_ok=True)\n",
    "    for k in tqdm(range(len(pred_verts))):\n",
    "        img = np.zeros((2160, 3840,3), np.uint8) + 255\n",
    "        # print(k)\n",
    "        pred_all_verts = np.vstack((pred_verts[k].numpy(),obj_verts[k]))\n",
    "        faces = np.vstack((hand_faces,obj_faces+778))\n",
    "        pred_img, _ = vis_smpl_color(render_out, img, k, pred_all_verts, faces,camera_pose[:3,:3],camera_pose[:3,3],K)\n",
    "        outname = os.path.join(render_out, '{:06d}.jpg'.format(k))\n",
    "        # print(outname)\n",
    "        cv2.imwrite(outname, pred_img)\n",
    "        # break\n",
    "    video_name = 'seq' + seq + '_view' + str(view) + '.mp4' \n",
    "    video_path = join(render_out,video_name)\n",
    "    img2video(render_out,video_path)\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:39<00:00,  8.83it/s]\n"
     ]
    }
   ],
   "source": [
    "video_name = 'seq' + seq + '_view' + str(view) + '.mp4' \n",
    "video_path = join(render_out,video_name)\n",
    "img2video(render_out,video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "render",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
