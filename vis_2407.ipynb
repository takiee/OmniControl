{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果可视化\n",
    "## 直接的kp点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from os.path import join\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取generate生成的result.npy\n",
    "path = 'save/0701_o2h_mid/samples_0701_o2h_mid_000200000_seed10_predefined/results.npy'\n",
    "res = np.load(path,allow_pickle=True).item()\n",
    "all_hand_motions = torch.tensor(res['all_hand_motions']) # \n",
    "all_obj_motions = torch.tensor(res['all_obj_motions']).reshape(-1,30,3,4) \n",
    "seqs = res['seqs']\n",
    "datapath = '/root/code/seqs/0303_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 30, 21, 3)\n"
     ]
    }
   ],
   "source": [
    "# all_hand_motions.shape\n",
    "hand_kp = all_hand_motions[:,:,:63].reshape(-1,30,21,3).numpy()\n",
    "print(hand_kp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_obj_motions.shape\n",
    "# seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    }
   ],
   "source": [
    "# 获得对应的物体点\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "seq = seqs[0]\n",
    "seq_path = join(datapath,seq)\n",
    "meta_path = join(seq_path,'meta.pkl')\n",
    "with open(meta_path,'rb')as f:\n",
    "    meta = pickle.load(f)\n",
    "active_obj = meta['active_obj']\n",
    "obj_verts = torch.tensor(np.load(join(obj_path,active_obj,'resampled_500_trans.npy'))).float() # 500,3\n",
    "obj_pose = all_obj_motions[0] # 30,3,4\n",
    "\n",
    "def get_all_frame_obj_verts(obj_verts, obj_pose):\n",
    "    # tensor, N,3   T,3,4\n",
    "    nf = obj_pose.shape[0]\n",
    "    obj_verts = obj_verts.unsqueeze(0).repeat(nf,1,1)\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    new_obj_verts = torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T\n",
    "    return new_obj_verts\n",
    "\n",
    "all_obj_verts = get_all_frame_obj_verts(obj_verts, obj_pose)\n",
    "print(all_obj_verts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "\n",
    "# 模拟你的手部运动数据 (30帧，21个关键点，每个关键点有3个坐标)\n",
    "hand_data = hand_kp[0]\n",
    "obj_data = all_obj_verts.numpy()\n",
    "\n",
    "# 获取关键点连接信息\n",
    "connections = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),       # 拇指\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),       # 食指\n",
    "    (0, 9), (9, 10), (10, 11), (11, 12),  # 中指\n",
    "    (0, 13), (13, 14), (14, 15), (15, 16), # 无名指\n",
    "    (0, 17), (17, 18), (18, 19), (19, 20)  # 小指\n",
    "]\n",
    "\n",
    "view = 1\n",
    "# 设置视频参数\n",
    "fps = 6\n",
    "frame_size = (3840, 2160)\n",
    "output_file = 'save/0701_o2h_mid/samples_0701_o2h_mid_000200000_seed10_predefined/0_hand_keypoints.mp4'\n",
    "calib_path = \"/root/code/seqs/calibration_all.json\"\n",
    "with open(calib_path) as f:\n",
    "    calib_dome = json.load(f)\n",
    "    f.close()\n",
    "camera_pose = np.vstack((np.asarray(calib_dome[str(view)]['RT']).reshape((3,4)), np.ones(4) ))\n",
    "K = np.asarray(calib_dome[str(view)]['K']).reshape((3,3))\n",
    "R = camera_pose[:3,:3]\n",
    "T = camera_pose[:3,3]\n",
    "\n",
    "\n",
    "# 初始化视频写入对象\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "# 创建一个空白图像\n",
    "def create_blank_image():\n",
    "    return np.ones((frame_size[1], frame_size[0], 3), dtype=np.uint8) * 255\n",
    "\n",
    "# 3D关键点到2D图像坐标的投影\n",
    "def project_points(points_3d, K, R, T):\n",
    "    points_3d_hom = np.hstack((points_3d, np.ones((points_3d.shape[0], 1))))  # 转换为齐次坐标\n",
    "    RT = np.hstack((R, T.reshape(-1, 1)))  # 组合旋转矩阵和平移向量\n",
    "    points_camera = RT @ points_3d_hom.T  # 投影到相机坐标系\n",
    "    points_camera = points_camera[:3, :] / points_camera[2, :]  # 归一化\n",
    "    points_image = K @ points_camera  # 投影到图像平面\n",
    "    return points_image[:2, :].T\n",
    "\n",
    "# 绘制关键点和连接线\n",
    "def draw_keypoints_and_connections(frame, keypoints):\n",
    "    for connection in connections:\n",
    "        pt1 = tuple(keypoints[connection[0]].astype(int))\n",
    "        pt2 = tuple(keypoints[connection[1]].astype(int))\n",
    "        cv2.line(frame, pt1, pt2, (0, 255, 0), 2)\n",
    "    for keypoint in keypoints:\n",
    "        cv2.circle(frame, tuple(keypoint.astype(int)), 5, (0, 0, 255), -1)\n",
    "    return frame\n",
    "\n",
    "# 绘制物体点\n",
    "def draw_object_points(frame, points):\n",
    "    for point in points:\n",
    "        cv2.circle(frame, tuple(point.astype(int)), 3, (255, 0, 0), -1)\n",
    "    return frame\n",
    "\n",
    "# 创建视频帧\n",
    "for i in range(hand_data.shape[0]):\n",
    "    img = create_blank_image()\n",
    "    hand_keypoints_2d = project_points(hand_data[i], K, R, T)\n",
    "    object_points_2d = project_points(obj_data[i], K, R, T)\n",
    "    frame = draw_keypoints_and_connections(img, hand_keypoints_2d)\n",
    "    frame = draw_object_points(frame, object_points_2d)\n",
    "    out.write(frame)\n",
    "\n",
    "# 释放视频写入对象\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将上述分段的代码整合起来：批量可视化生成结果中的hand kp和object points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 30, 21, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/81 [00:01<01:45,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/81 [00:02<01:53,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/81 [00:04<01:47,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4/81 [00:05<01:43,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/81 [00:06<01:42,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/81 [00:08<01:38,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 7/81 [00:09<01:37,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 8/81 [00:10<01:38,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 9/81 [00:12<01:35,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 10/81 [00:13<01:33,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 11/81 [00:14<01:31,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 12/81 [00:15<01:28,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 13/81 [00:17<01:28,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 14/81 [00:18<01:27,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 15/81 [00:19<01:25,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 16/81 [00:21<01:23,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 17/81 [00:22<01:22,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 18/81 [00:23<01:20,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 19/81 [00:24<01:20,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 20/81 [00:26<01:19,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 21/81 [00:27<01:18,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 22/81 [00:28<01:16,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 23/81 [00:30<01:15,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 24/81 [00:31<01:13,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 25/81 [00:32<01:13,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 26/81 [00:34<01:12,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 27/81 [00:35<01:10,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 28/81 [00:36<01:08,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 29/81 [00:37<01:06,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 30/81 [00:39<01:05,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 31/81 [00:40<01:04,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 32/81 [00:41<01:03,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 33/81 [00:43<01:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 34/81 [00:44<01:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 35/81 [00:45<00:59,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 36/81 [00:46<00:57,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 37/81 [00:48<00:57,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 38/81 [00:49<00:56,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 39/81 [00:50<00:54,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 500, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 39/81 [00:51<00:55,  1.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m     frame \u001b[38;5;241m=\u001b[39m draw_keypoints_and_connections(img, hand_keypoints_2d)\n\u001b[1;32m    115\u001b[0m     frame \u001b[38;5;241m=\u001b[39m draw_object_points(frame, object_points_2d)\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# 释放视频写入对象\u001b[39;00m\n\u001b[1;32m    119\u001b[0m out\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from os.path import join\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import *\n",
    "\n",
    "# 创建一个空白图像\n",
    "def create_blank_image():\n",
    "    return np.ones((frame_size[1], frame_size[0], 3), dtype=np.uint8) * 255\n",
    "\n",
    "# 3D关键点到2D图像坐标的投影\n",
    "def project_points(points_3d, K, R, T):\n",
    "    points_3d_hom = np.hstack((points_3d, np.ones((points_3d.shape[0], 1))))  # 转换为齐次坐标\n",
    "    RT = np.hstack((R, T.reshape(-1, 1)))  # 组合旋转矩阵和平移向量\n",
    "    points_camera = RT @ points_3d_hom.T  # 投影到相机坐标系\n",
    "    points_camera = points_camera[:3, :] / points_camera[2, :]  # 归一化\n",
    "    points_image = K @ points_camera  # 投影到图像平面\n",
    "    return points_image[:2, :].T\n",
    "\n",
    "# 绘制关键点和连接线\n",
    "def draw_keypoints_and_connections(frame, keypoints):\n",
    "    for connection in connections:\n",
    "        pt1 = tuple(keypoints[connection[0]].astype(int))\n",
    "        pt2 = tuple(keypoints[connection[1]].astype(int))\n",
    "        cv2.line(frame, pt1, pt2, (0, 255, 0), 2)\n",
    "    for keypoint in keypoints:\n",
    "        cv2.circle(frame, tuple(keypoint.astype(int)), 5, (0, 0, 255), -1)\n",
    "    return frame\n",
    "\n",
    "# 绘制物体点\n",
    "def draw_object_points(frame, points):\n",
    "    for point in points:\n",
    "        cv2.circle(frame, tuple(point.astype(int)), 3, (255, 0, 0), -1)\n",
    "    return frame\n",
    "\n",
    "def get_all_frame_obj_verts(obj_verts, obj_pose):\n",
    "    # tensor, N,3   T,3,4\n",
    "    nf = obj_pose.shape[0]\n",
    "    obj_verts = obj_verts.unsqueeze(0).repeat(nf,1,1)\n",
    "    # return obj_verts\n",
    "    obj_R = obj_pose[:,:3,:3]\n",
    "    obj_R = torch.einsum('...ij->...ji', [obj_R])\n",
    "    obj_T = obj_pose[:,:3,3].unsqueeze(1)\n",
    "    new_obj_verts = torch.einsum('fpn,fnk->fpk',obj_verts,obj_R) + obj_T\n",
    "    return new_obj_verts\n",
    "\n",
    "\n",
    "path = 'save/0701_o2h_mid/samples_0701_o2h_mid_000200000_seed10_predefined/results.npy'\n",
    "obj_path = '/root/code/seqs/object/'\n",
    "res = np.load(path,allow_pickle=True).item()\n",
    "all_hand_motions = torch.tensor(res['all_hand_gt']) \n",
    "# all_hand_motions = torch.tensor(res['all_hand_motions']) \n",
    "all_obj_motions = torch.tensor(res['all_obj_motions']).reshape(-1,30,3,4) \n",
    "seqs = res['seqs']\n",
    "datapath = '/root/code/seqs/0303_data/'\n",
    "\n",
    "hand_kp = all_hand_motions[:,:,:63].reshape(-1,30,21,3).numpy()\n",
    "print(hand_kp.shape)\n",
    "\n",
    "for i in tqdm(range(hand_kp.shape[0])):\n",
    "    # 获得对应的物体点\n",
    "    seq = seqs[i] \n",
    "    seq_path = join(datapath,seq)\n",
    "    meta_path = join(seq_path,'meta.pkl')\n",
    "    with open(meta_path,'rb')as f:\n",
    "        meta = pickle.load(f)\n",
    "    active_obj = meta['active_obj']\n",
    "    obj_verts = torch.tensor(np.load(join(obj_path,active_obj,'resampled_500_trans.npy'))).float() # 500,3\n",
    "\n",
    "    obj_pose = all_obj_motions[i] # 30,3,4\n",
    "    all_obj_verts = get_all_frame_obj_verts(obj_verts, obj_pose)\n",
    "    print(all_obj_verts.shape)\n",
    "\n",
    "    # 可视化\n",
    "    # 模拟你的手部运动数据 (30帧，21个关键点，每个关键点有3个坐标)\n",
    "    hand_data = hand_kp[i]\n",
    "    obj_data = all_obj_verts.numpy()\n",
    "\n",
    "    # 获取关键点连接信息\n",
    "    connections = [\n",
    "        (0, 1), (1, 2), (2, 3), (3, 4),       # 拇指\n",
    "        (0, 5), (5, 6), (6, 7), (7, 8),       # 食指\n",
    "        (0, 9), (9, 10), (10, 11), (11, 12),  # 中指\n",
    "        (0, 13), (13, 14), (14, 15), (15, 16), # 无名指\n",
    "        (0, 17), (17, 18), (18, 19), (19, 20)  # 小指\n",
    "    ]\n",
    "\n",
    "    view = 1\n",
    "    # 设置视频参数\n",
    "    fps = 6\n",
    "    frame_size = (3840, 2160)\n",
    "    output_file = f'save/0701_o2h_mid/samples_0701_o2h_mid_000200000_seed10_predefined/{str(i)}_gt.mp4'\n",
    "    calib_path = \"/root/code/seqs/calibration_all.json\"\n",
    "    with open(calib_path) as f:\n",
    "        calib_dome = json.load(f)\n",
    "        f.close()\n",
    "    camera_pose = np.vstack((np.asarray(calib_dome[str(view)]['RT']).reshape((3,4)), np.ones(4) ))\n",
    "    K = np.asarray(calib_dome[str(view)]['K']).reshape((3,3))\n",
    "    R = camera_pose[:3,:3]\n",
    "    T = camera_pose[:3,3]\n",
    "\n",
    "    # 初始化视频写入对象\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "\n",
    "    # 创建视频帧\n",
    "    for i in range(hand_data.shape[0]):\n",
    "        img = create_blank_image()\n",
    "        hand_keypoints_2d = project_points(hand_data[i], K, R, T)\n",
    "        object_points_2d = project_points(obj_data[i], K, R, T)\n",
    "        frame = draw_keypoints_and_connections(img, hand_keypoints_2d)\n",
    "        frame = draw_object_points(frame, object_points_2d)\n",
    "        out.write(frame)\n",
    "\n",
    "    # 释放视频写入对象\n",
    "    out.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "render",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
